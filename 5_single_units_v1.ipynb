{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise session 5: Models of the visual system\n",
    "\n",
    "During the lecture we studied how the responses of single neurons can inform us about the way the visual system works. In this exercise session, we will apply a similar approach to models of the visual system, to try to answer the question: <b>What is a good model of the brain's visual system?</b>\n",
    "\n",
    "Throughout this notebook, you will find out at the beginning of each cell whether you need to adjust a parameter or just run the cell as it is. Below each assignment you will also find <span style=\"color: green; font-weight: bold;\">hints</span> on how to think about the question or pointers to where it was discussed in the lecture.\n",
    "\n",
    "Run the next cell to import the necessary code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without changing\n",
    "\n",
    "!mkdir utils_ex5\n",
    "!wget -P utils_ex5/ https://github.com/ManteLab/Iton_notebooks_public/raw/refs/heads/main/utils_ex5/vgg_utils.py\n",
    "!wget -P utils_ex5/ https://github.com/ManteLab/Iton_notebooks_public/raw/refs/heads/main/utils_ex5/rao_ballard_trained.pkl\n",
    "\n",
    "from utils_ex5.vgg_utils import *\n",
    "\n",
    "# Import packages\n",
    "import torch #machine learning package\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, IntSlider, FloatSlider\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents:\n",
    "\n",
    "* [Part I: Convolution neural networks as a model of the visual system](#cnns)\n",
    "    * [1: Receptive Fields of CNNs](#rfs_cnns)\n",
    "    * [2: Stimuli: Sinusoidal gratings](#gratings)\n",
    "    * [3: Presenting stimuli to CNNs](#stimuli_cnns)\n",
    "    * [4: Contrast saturation](#contrast_cnns)\n",
    "    * [5: Cross-orientation suppression](#inhibtion_cnns)\n",
    "    * [6: Contrast saturation for optimal vs. sup-optimal orientation](#contrast_cnns_2)\n",
    "    \n",
    "\n",
    "* [Part II: Rao and Ballard model of predictive coding in V1](#rao_ballard)\n",
    "    * [1: Contrast saturation](#contrast_rao)\n",
    "    * [2: Cross-orientation suppression](#inhibtion_rao)\n",
    "    * [3: End-inhibiton](#end_inh_rao)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I. Convolutional neural networks <a class=\"anchor\" id=\"cnns\"></a>\n",
    "\n",
    "Convolutional neural networks (CNNs) are artificial neural network models that classify images by detecting what objects are inside the image. CNNs work by applying many \"layers\" of operations to the image to extract important information (below: feature learning) where \"neurons\" are the nodes in each layer. In a way, this is somewhat similar to the way the brain works by first computing simple properties of the world in primary visual cortex, and then building to complex representations in higher visual cortex. CNNs also work remarkably well as image classifiers, no other models come as close in accuracy to humans, so maybe they are a good model of the brain. In this exercise session we will try to find out.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/ManteLab/Iton_notebooks_public/refs/heads/main/images/cnn_architecture.png \"Convolutional neural networks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment 1** Assuming that CNNs are a good model of the brain, what sort of \"neurons\" would we find in the first layers of the CNN?\n",
    "\n",
    "<details>\n",
    "<summary style=\"color: green; font-weight: bold;\">Hint</summary>\n",
    "\n",
    "The first layers of the CNN we might find neurons similar to those of cells with center-surround cells like in the Thalamus (LGN). Alternatively, we might find \"neurons\" that respond to simple features like orientations and contrast like in V1.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load a convolutional neural network (we are using a network called Alexnet, more info: [here](https://pytorch.org/hub/pytorch_vision_alexnet/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without changing\n",
    "\n",
    "convolutional_neural_net = load_cnn_model()\n",
    "layers = get_network_layers(convolutional_neural_net)\n",
    "\n",
    "print('Below we display the architecture:')\n",
    "print(convolutional_neural_net)\n",
    "\n",
    "print('Our goal is to study how the layers of the feature learning part, i.e. (0) to (12), transform the input image.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Receptive Fields of the *convolutional layers*<a class=\"anchor\" id=\"rfs_cnns\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without changing\n",
    "\n",
    "plot_alexnet_rfs(convolutional_neural_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment 2** What happens with the filters when you go into deeper layers?\n",
    "\n",
    "<details>\n",
    "<summary style=\"color: green; font-weight: bold;\">Hint</summary>\n",
    "\n",
    "When you go into deeper layers in a neural network, the filters perform more abstract and high-level transformations on the input data.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stimuli: sinusoidal gratings <a class=\"anchor\" id=\"gratings\"></a>\n",
    "\n",
    "In this exercise session you will act like a neuroscientist investigating a real brain. You'll start simple: by investigating the network's response to very simple stimuli: the gratings seen in the lecture. Run the next piece of code to explore the parameters of the gratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without changing\n",
    "\n",
    "interact(interactive_grating, \n",
    "    frequency=FloatSlider(min=2, max=20, step=0.5, value=10, description=\"Frequency (Hz)\",continuous_update=False),\n",
    "    radius=IntSlider(min=1, max=100, step=10, value=12, description=\"Radius\", continuous_update=False),\n",
    "    contrast=FloatSlider(min=0.1, max=1., step=0.05, value=1, description=\"Contrast\", continuous_update=False),\n",
    "    orientation=IntSlider(min=0, max=180, step=12, value=90, description=\"Orientation\", continuous_update=False),\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following sections we will perform a series of experiments on the CNN \"neurons\" to quantify their similarity to properties of real neurons in the visual system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Presenting stimuli to CNNs <a class=\"anchor\" id=\"stimuli_cnns\"></a>\n",
    "\n",
    "Just like the lecture, we will present different grating stimuli to the different \"neurons\" of the network. We will start by presenting gratings with different levels of contrast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without changing\n",
    "\n",
    "# Set parameters\n",
    "size = (256, 256)\n",
    "center_x = 125\n",
    "center_y = 125 \n",
    "frequency = 15\n",
    "orientation = 90\n",
    "radius = 50\n",
    "\n",
    "\n",
    "#generate different gratings with increasing contrasts\n",
    "contrasts = np.linspace(0.1, 1, 10)\n",
    "gratings = create_contrast_gratings(frequency, radius, contrasts, orientation)\n",
    "\n",
    "plot_image_row(gratings, titles=[f'contrast: {c:.2}' for c in contrasts])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next: we'll pass the gratings through the convolutional neural network.\n",
    " \n",
    "We will investigate what types of computations are performed at the different layers. For this, we choose the number of layers we want to keep by setting the variable NUM_LAYERS and run a series of subsequent analyses to see how the output of the network looks like after NUM_LAYERS layers and how the output changes as we change the inputs.\n",
    "\n",
    "We start with NUM_LAYERS = 1, which is the output of the (1): ReLU(inplace=True).\n",
    "\n",
    "Run the following cell a few times, changing the parameter NUM_LAYERS, and look at 1) how the dimensionality of variable network_outputs changes and 2) how the network's output, i.e. neuron_response, changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you can change NUM_LAYERS variable to adjust the number of computations\n",
    "\n",
    "print(f'The size of the input (gratings variable) is {gratings[0].shape[1]} x {gratings[0].shape[2]}.')\n",
    "\n",
    "#This parameter determines how many layers of computation we are applying to the input image.\n",
    "NUM_LAYERS = 1 # possible values: 1, 2, 3, 4, 5\n",
    "NEURON_NUMBER = 0\n",
    "\n",
    "# You don't need to change anything below here.\n",
    "network_layers = layers[0:NUM_LAYERS+1]\n",
    "print('We are passing the input through the following layers: ')\n",
    "print(network_layers)\n",
    "\n",
    "#pass the gratings through the layers we selected\n",
    "network_outputs = network_layers(gratings).detach()\n",
    "\n",
    "print(f'The variable network_outputs has dimensionality {network_outputs.shape[0]} stimuli x {network_outputs.shape[1]} x {network_outputs.shape[2]} x {network_outputs.shape[3]} \"neurons\"')\n",
    "\n",
    "neuron_response = network_outputs[:, NEURON_NUMBER]\n",
    "\n",
    "id_x, id_y = get_center(network_outputs.shape[2], network_outputs.shape[3])\n",
    "\n",
    "#plot the responses of our neuron in the layer we selected\n",
    "plot_image_row(neuron_response.unsqueeze(1), titles=[f'c: {c:.2}' for c in contrasts])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment 3** What happens to the resulted image when you apply more layers of computation? Run the previous cell and change the NUM_LAYERS. What happens before and after MaxPool2d?\n",
    "\n",
    "<details>\n",
    "<summary style=\"color: green; font-weight: bold;\">Hint</summary>\n",
    "\n",
    "The early layers \"highlight\" basic components of the image without altering the overall structure, while the deeper layers do not \"see\" the image in the way humans would; instead, they encode the features in a way that makes sense for classification or decision-making purposes. Before MaxPool2d the output is sparse, while after MaxPool2d the output is more dense. Look at Slide 39 of Lecture 5. How does MaxPool2d relate to what a complex cell does?\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiment 1: Contrast saturation <a class=\"anchor\" id=\"contrast_cnns\"></a>\n",
    "\n",
    "This experiment was mentioned in class in \"Non-linear computations in V1\", Slide 40 of Lecture 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we will plot the activity of each neuron as a function of different stimuli contrasts. The orientation of the stimuli is fixed across the different contrasts. Each line illustrates one neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without changing\n",
    "\n",
    "print(f'The variable network_outputs has dimensionality {network_outputs.shape[0]} stimuli x {network_outputs.shape[1]} \"neurons\"  x {network_outputs.shape[2]} \"neurons\" x {network_outputs.shape[3]} \"neurons\"')\n",
    "\n",
    "_=plt.plot(contrasts, network_outputs[:, :, id_x, id_y].detach().numpy())\n",
    "\n",
    "plt.xlabel('Stimuli contrasts')\n",
    "plt.ylabel('Neuron Response')\n",
    "\n",
    "# Optionally set a title\n",
    "plt.title('Neuron Response vs. Stimuli Contrasts')\n",
    "\n",
    "# Show the plot\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiment 2: Cross-orientation suppression <a class=\"anchor\" id=\"inhibtion_cnns\"></a>\n",
    "\n",
    "This experiment was mentioned in class in \"Non-linear computations in V1\", Slide 40 of Lecture 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find preferred orientation for each neuron\n",
    "\n",
    "The first step is to identify for each neuron its preferred orientation. We will do this by computing each neuron's response to gratings of different orientations. We consider the orientation that elicits the highest response as being the neuron's preferred orientation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without changing\n",
    "\n",
    "# Set parameters\n",
    "size = 256\n",
    "center = (125, 125)\n",
    "\n",
    "radius = 50\n",
    "frequency = 10\n",
    "contrast = 1\n",
    "\n",
    "orientations = np.linspace(0, 170, 10)\n",
    "\n",
    "gratings = create_orientation_gratings(frequency, radius, contrast, orientations, size, center)\n",
    "plot_image_row(gratings, titles=[f'orientation: {o:.0f}' for o in orientations])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will plot the activity of each neuron as a function of different stimuli orientations. The contrast is now fixed. Each line illustrates one neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without changing\n",
    "\n",
    "network_layers = layers[0:NUM_LAYERS+1]\n",
    "network_outputs = network_layers(gratings)\n",
    "\n",
    "_=plt.plot(orientations, network_outputs[:, :, id_x, id_y].detach().numpy())\n",
    "\n",
    "# Set the x-axis and y-axis labels\n",
    "plt.xlabel('Stimuli Orientations [deg]')\n",
    "plt.ylabel('Neuron Response')\n",
    "\n",
    "# Optionally set a title\n",
    "plt.title('Neuron Response vs. Stimuli Orientations')\n",
    "\n",
    "# Show the plot\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment 4** How do we determine a neuron's preferred orientation?\n",
    "\n",
    "<details>\n",
    "<summary style=\"color: green; font-weight: bold;\">Hint</summary>\n",
    "\n",
    "We will determine each neuron's preferred orientation by looking which stimuli orientation elicited the highest response.\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without changing\n",
    "\n",
    "print(f'The output of the network after passing an input image through {NUM_LAYERS} layers is: {network_outputs.shape[0]} stimuli x {network_outputs.shape[1]} x {network_outputs.shape[2]} x {network_outputs.shape[3]} neurons')\n",
    "\n",
    "m = network_outputs[:, :, id_x, id_y].detach().numpy()\n",
    "\n",
    "print(f'The tuning curve of each neuron is stored in variable m of dimensionality {m.shape[0]} stimuli x {m.shape[1]} neurons')\n",
    "\n",
    "# We take the maximum across rows, i.e. for each neuron we look which stimuli elicited the highest response\n",
    "max_indices = np.argmax(m, axis=0) \n",
    "neuron_pref_orientation = orientations[max_indices]\n",
    "\n",
    "print(f'Each neurons preferred orientation is stored in variable neuron_pref_orientation of dimensionality {neuron_pref_orientation.shape[0]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment 5** Inspecting the figure above: are all neurons behaving similarly?\n",
    "\n",
    "<details>\n",
    "<summary style=\"color: green; font-weight: bold;\">Hint</summary>\n",
    "\n",
    "Different neurons have different preferred orientations, i.e. their highest response is elicited by different stimuli. Moreover, some neurons are tuned to orientation (their activity changes as a function of the orientations), while some neurons are untuned (their activity is invariant to the orientation).\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "**Assignment 6** How could we select only the neurons that are tuned?\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary style=\"color: green; font-weight: bold;\">Hint</summary>\n",
    "\n",
    "One possibility is to compute the variance across stimuli orientations. The higher the variance, the better the tuning of the neuron.\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without changing\n",
    "\n",
    "variances = np.var(m, axis=0)\n",
    "\n",
    "# Plot the histogram of the variances\n",
    "plt.hist(variances, bins=20, edgecolor='black')\n",
    "plt.title('Histogram of Variances')\n",
    "plt.xlabel('Variance across Orientations')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Super-imposing two gratings: grating of preferred orientation and grating of orthogonal orientation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each neuron, we compute its response to its optimal grating superimposed with an orthogonal grating of varying contrasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we select an *arbitrary* threshold to focus only on the most tuned units\n",
    "\n",
    "variance_threshold = np.median(variances)\n",
    "valid_neurons = variances > variance_threshold \n",
    "valid_neuron_indices = np.nonzero(valid_neurons)[0]\n",
    "\n",
    "print('A list of tuned units: ', valid_neuron_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without changing\n",
    "\n",
    "pref_orientation, orth_orientation = get_pref_and_orth_angle(neuron_pref_orientation[valid_neuron_indices[NEURON_NUMBER]])\n",
    "\n",
    "print(f'Prefered orientation of neuron {NEURON_NUMBER} is {pref_orientation:.0f} deg and orth orientation is {orth_orientation:.0f} deg.')\n",
    "\n",
    "# Set parameters\n",
    "size = 256\n",
    "center = (125, 125)\n",
    "radius = 50\n",
    "frequency = 10\n",
    "contrast = 0.5\n",
    "\n",
    "orientations = np.linspace(0, 170, 10)\n",
    "contrast_pref = np.linspace(0.5, 0.5, 1)\n",
    "contrasts_orth = np.linspace(0.1, 0.5, 10)\n",
    "\n",
    "print(f'Orthogonal orientation {orth_orientation:.0f}')\n",
    "gratings_orthogonal = create_contrast_gratings(frequency, radius, contrasts_orth, orth_orientation)\n",
    "plot_image_row(gratings_orthogonal, titles=[f'contrast: {c:.2}' for c in contrasts_orth])\n",
    "\n",
    "print('Prefered and orthogonal orientation super-imposed')\n",
    "gratings_superimposed = create_superimposed_gratings(frequency, radius, contrast_pref, contrasts_orth, pref_orientation, orth_orientation, size, center)\n",
    "plot_image_row(gratings_superimposed, titles=[f'contrast: {c:.2}' for c in contrasts_orth])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without changing\n",
    "\n",
    "# We pass the two types of stimuli (orthogonal and super-imposed through the network)\n",
    "network_layers = layers[0:NUM_LAYERS+1]\n",
    "network_outputs_superimposed = network_layers(gratings_superimposed)\n",
    "network_outputs_orthogonal = network_layers(gratings_orthogonal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response of a neuron to super-imposed gratings (one contrast value for preferred orientation grating)\n",
    "\n",
    "The response of a neuron when we super-impose a grating of orthogonal direction of varying contrasts onto a grating of its preferred orientation and high contrast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without changing\n",
    "\n",
    "_=plt.plot(network_outputs_superimposed[:, valid_neuron_indices[NEURON_NUMBER],id_x,id_y].detach().numpy(), label = 'super-imposed stimuli')\n",
    "_=plt.plot(network_outputs_orthogonal[:, valid_neuron_indices[NEURON_NUMBER],id_x,id_y].detach().numpy(), label = 'orthogonal stimulus')\n",
    "\n",
    "\n",
    "plt.xlabel('Contrast Values of Orthogonal Orientation')\n",
    "plt.ylabel('Neuron Response')\n",
    "plt.legend()\n",
    "# Show the plot\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment 7** How does the neuron respond to the orthogonal grating? How does the neuron respond when we super-impose the orthogonal grating onto the preferred grating? How does this compare to the response of a V1 neuron?\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary style=\"color: green; font-weight: bold;\">Hint</summary>\n",
    "\n",
    "The neuron does not respond to an orthogonal grating. However, when the orthogonal grating is super-imposed to the preferred grating, it suppresses the neuron's response to the preferred grating. This effect was mentioned in class in \"Non-linear computations in V1\", Slide 40 of Lecture 5.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response of a neuron to super-imposed gratings (all contrast values for preferred orientation grating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without changing\n",
    "\n",
    "contrasts_orth = np.linspace(0.1, 0.5, 10)\n",
    "contrasts_pref_sweep = np.linspace(0.1, 0.5, 4)\n",
    "\n",
    "for c in contrasts_pref_sweep:\n",
    "    contrast_pref = np.linspace(c, c, 1)\n",
    "    gratings = create_superimposed_gratings(frequency, radius, contrast_pref, contrasts_orth, pref_orientation, orth_orientation, size, center)\n",
    "\n",
    "    network_layers = layers[0:NUM_LAYERS+1]\n",
    "    network_outputs = network_layers(gratings)\n",
    "    _=plt.plot(contrasts_orth, network_outputs[:, valid_neuron_indices[NEURON_NUMBER],id_x,id_y].detach().numpy(), label=f'Contrast: {c:.2f}')\n",
    "\n",
    "\n",
    "plt.xlabel('Contrast Values of Orthogonal Orientation')\n",
    "plt.ylabel('Neuron Response')\n",
    "\n",
    "# Show the legend\n",
    "plt.legend(title='Contrast Values for Prefered Orientation')\n",
    "# Show the plot\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment 8** Similarly to Assignment 7, how does the response to an optimal orientation change when we add an orthogonal grating? \n",
    "\n",
    "\n",
    "<details>\n",
    "<summary style=\"color: green; font-weight: bold;\">Hint</summary>\n",
    "\n",
    "Along the y-axis, we can keep the contrast of the orthogonal grating fixed and compare the neuron's response to the preferred grating with different contrasts (red for high contrast and blue for low contrast). Each line then shows a decrease, replicating the cross-orientation suppression experiment for different levels of contrast for the preferred grating. \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without changing\n",
    "\n",
    "contrasts_orth_sweep = np.linspace(0.1, 0.5, 4)\n",
    "contrasts_pref = np.linspace(0.1, 0.5, 10)\n",
    "\n",
    "for c in contrasts_orth_sweep:\n",
    "    contrast_orth = np.linspace(c, c, 1)\n",
    "    gratings = create_superimposed_gratings(frequency, radius, contrast_orth, contrasts_pref, orth_orientation, pref_orientation, size, center)\n",
    "\n",
    "    network_layers = layers[0:NUM_LAYERS+1]\n",
    "    network_outputs = network_layers(gratings)\n",
    "    _=plt.plot(contrasts_pref, network_outputs[:, valid_neuron_indices[NEURON_NUMBER],id_x,id_y].detach().numpy(), label=f'Contrast: {c:.2f}')\n",
    "\n",
    "\n",
    "plt.xlabel('Contrast Values of Preferred Orientation')\n",
    "plt.ylabel('Neuron Response')\n",
    "\n",
    "# Show the legend\n",
    "plt.legend(title='Contrast Values for Orthogonal Orientation')\n",
    "# Show the plot\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This experiment is the same as the one before, just now each line represents a different level of contrast for the orthogonal grating. This plot aims to replicate the effect mentioned in class in \"Non-linear computations in V1\", Slide 40 of Lecture 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Contrast saturation for optimal vs. sup-optimal orientation <a class=\"anchor\" id=\"contrast_cnns_2\"></a>\n",
    "\n",
    "This effect was mentioned in class in \"Non-linear computations in V1\" - contrast saturation, Slide 40 of Lecture 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without changing\n",
    "\n",
    "pref_orientation, orth_orientation = get_pref_and_orth_angle(neuron_pref_orientation[valid_neuron_indices[NEURON_NUMBER]])\n",
    "\n",
    "suboptimal_orientation = pref_orientation - 15\n",
    "\n",
    "# Set parameters\n",
    "size = (256, 256)\n",
    "center_x = 125\n",
    "center_y = 125 \n",
    "frequency = 7.5\n",
    "radius = 50\n",
    "\n",
    "contrasts = np.linspace(0.1, 1, 10)\n",
    "\n",
    "\n",
    "#generate different gratings with increasing contrasts\n",
    "gratings = create_contrast_gratings(frequency, radius, contrasts, pref_orientation)\n",
    "#select the layers of the network we want to pass the gratings through\n",
    "network_layers = layers[0:NUM_LAYERS+1]\n",
    "#pass the gratings through the layers we selected\n",
    "#this matrix has the following shape: contrasts x 2d neuron response\n",
    "network_outputs_optimal = network_layers(gratings).detach()\n",
    "\n",
    "\n",
    "response_optimal = network_outputs_optimal[:, valid_neuron_indices[NEURON_NUMBER], id_x, id_y]\n",
    "gratings = create_contrast_gratings(frequency, radius, contrasts, suboptimal_orientation)\n",
    "#select the layers of the network we want to pass the gratings through\n",
    "network_layers = layers[0:NUM_LAYERS+1]\n",
    "#pass the gratings through the layers we selected\n",
    "#this matrix has the following shape: contrasts x 2d neuron response\n",
    "network_outputs_suboptimal = network_layers(gratings).detach()\n",
    "response_suboptimal = network_outputs_suboptimal[:, valid_neuron_indices[NEURON_NUMBER],id_x,id_y]\n",
    "\n",
    "\n",
    "_=plt.plot(contrasts, response_suboptimal.detach().numpy(), label='sub-optimal orientation')\n",
    "_=plt.plot(contrasts, response_optimal.detach().numpy(), label='optimal orientation')\n",
    "\n",
    "# Set the x-axis and y-axis labels\n",
    "plt.xlabel('Contrasts')\n",
    "plt.ylabel('Neuron Response')\n",
    "\n",
    "# Optionally set a title\n",
    "plt.title('Neuron Response vs. Contrasts')\n",
    "plt.legend(title='Stimulus orientations')\n",
    "\n",
    "# Show the plot\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Tuning to stimulus-size <a class=\"anchor\" id=\"size_cnns_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment 9** How does the response of a neuron change with stimulus size?\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary style=\"color: green; font-weight: bold;\">Hint</summary>\n",
    "\n",
    "For some neurons the response first increases with stimulus size and then it decreases. This is a non-classical receptive field effect. This effect was mentioned in class in \"Surround suppression in V1\" slide 42 and \"Multiple contributions to V1 responses\" slide 43 in Lecture 5.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without changing\n",
    "\n",
    "# Set parameters\n",
    "size = 256\n",
    "center = (125, 125)\n",
    "\n",
    "frequency = 10\n",
    "contrast = 1\n",
    "orientation = 0\n",
    "\n",
    "radii = np.linspace(10, 50, 10)\n",
    "\n",
    "gratings = create_radius_gratings(frequency, radii, contrast, orientation, size, center)\n",
    "plot_image_row(gratings, titles=[f'radius: {r:.0f}' for r in radii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without changing\n",
    "\n",
    "network_layers = layers[0:NUM_LAYERS+1]\n",
    "network_outputs = network_layers(gratings)\n",
    "\n",
    "_=plt.plot(orientations, network_outputs[:, :, id_x, id_y].detach().numpy())\n",
    "\n",
    "# Set the x-axis and y-axis labels\n",
    "plt.xlabel('Stimulus Size (radius)')\n",
    "plt.ylabel('Neuron Response')\n",
    "\n",
    "# Optionally set a title\n",
    "plt.title('Neuron Response vs. Stimulus Size')\n",
    "\n",
    "# Show the plot\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have run all these experiments on one layer (NUM_LAYERS = 1), go back to where this variable is set and see how the results change as you go deeper into the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II. Rao and Ballard model of predictive coding in V1 <a class=\"anchor\" id=\"rao_ballard\"></a>\n",
    "\n",
    "This experiment was mentioned in class in \"Predictive Coding\", Slide 48 in Lecture 5.\n",
    "\n",
    "[Rajesh P. N. Rao & Dana H. Ballard Nature 1999 - Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects.](https://www.nature.com/articles/nn0199_79)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without changing\n",
    "\n",
    "#load the trained rao and ballard model\n",
    "model = load_rao_ballard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without changing\n",
    "\n",
    "# Set parameters\n",
    "frequency = 3\n",
    "orientation = 90\n",
    "radius = 6\n",
    "\n",
    "\n",
    "#generate different gratings with increasing contrasts\n",
    "contrasts = np.linspace(0.1, 1, 10)\n",
    "gratings = create_contrast_gratings(frequency, radius, contrasts, orientation, size=16, center=(8, 8))\n",
    "plot_image_row(gratings, titles=[f'c: {c:.2f}' for c in contrasts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without changing\n",
    "\n",
    "model.initialize_states(gratings[0].reshape(3, 256).numpy())\n",
    "\n",
    "layer_1_activities_per_contrast = np.zeros((len(gratings), 32))\n",
    "layer_2_activities_per_contrast = np.zeros((len(gratings), 128))\n",
    "\n",
    "for i, g in enumerate(gratings):\n",
    "    #the model is expecting a flattened representation of the image\n",
    "    er1, er2, l1, l2 = model(g.reshape(3, 256).numpy())\n",
    "    layer_1_activities_per_contrast[i] = abs(l1).mean(0)\n",
    "    layer_2_activities_per_contrast[i] = abs(l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Contrast saturation <a class=\"anchor\" id=\"contrast_rao\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment 10** Do neurons in layer 1 and layer 2 show contrast saturation?\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary style=\"color: green; font-weight: bold;\">Hint</summary>\n",
    "\n",
    "Neurons in layer 1 show contrast saturation while neurons in layer 2 are contrast-invariant (scale is very small). Contrast-saturation was mentioned in class in \"Non-linear computations in V1\", Slide 40 of Lecture 5.  \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can change layer_1_activities_per_contrast to layer_2_activities_per_contrast to see how layer 2 neurons change their response with contrast.\n",
    "\n",
    "_=plt.plot(contrasts, layer_1_activities_per_contrast)\n",
    "plt.xlabel('Contrast Values')\n",
    "plt.ylabel('Neuron response')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Cross-orientation suppression <a class=\"anchor\" id=\"inhibtion_rao\"></a>\n",
    "\n",
    "We will repeat this experiment similarly to the one in section (5) of Alexnet. First we determine for each neuron its preferred orientation. Then we select a neuron and we create a super-imposed grating consisting of its preferred grating and its orthogonal grating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without changing\n",
    "\n",
    "# Set parameters\n",
    "frequency = 3\n",
    "radius = 6\n",
    "contrast = 1\n",
    "\n",
    "orientations = np.linspace(0, 170, 10)\n",
    "\n",
    "gratings = create_orientation_gratings(frequency, radius, contrast, orientations, size=16, center=(8, 8))\n",
    "plot_image_row(gratings, titles=[f'orientation: {o:.0f}' for o in orientations])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without changing\n",
    "\n",
    "model.initialize_states(gratings[0].reshape(3, 256).numpy())\n",
    "\n",
    "layer_1_activities_per_orientation = np.zeros((len(gratings), 32))\n",
    "layer_2_activities_per_orientation = np.zeros((len(gratings), 128))\n",
    "\n",
    "for i, g in enumerate(gratings):\n",
    "    #the model is expecting a flattened representation of the image\n",
    "    er1, er2, l1, l2 = model(g.reshape(3, 256).numpy())\n",
    "    layer_1_activities_per_orientation[i] = abs(l1).mean(0)\n",
    "    layer_2_activities_per_orientation[i] = abs(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without changing\n",
    "\n",
    "_=plt.plot(orientations, layer_1_activities_per_orientation)\n",
    "plt.xlabel('orientations')\n",
    "plt.ylabel('average activity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without changing\n",
    "\n",
    "m = layer_1_activities_per_orientation\n",
    "print('Number of Stimuli Orientations x Number of Neurons: ', m.shape) # Number of Stimuli Orientations x Number of Neurons\n",
    "max_indices = np.argmax(m, axis=0) # we take the maximum across rows, i.e. for each neuron we look which stimuli elicited the highest response\n",
    "neuron_pref_orientation = orientations[max_indices]\n",
    "print('Number of Neurons: ', neuron_pref_orientation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without changing\n",
    "\n",
    "variances = np.var(m, axis=0)\n",
    "\n",
    "# Plot the histogram of the variances\n",
    "plt.hist(variances, bins=20, edgecolor='black')\n",
    "plt.title('Histogram of Variances')\n",
    "plt.xlabel('Variance across Orientations')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we select an *arbitrary* threshold to focus only on the most tuned units\n",
    "\n",
    "variance_threshold = np.median(variances)\n",
    "valid_neurons = variances > variance_threshold # we select an *arbitrary* threshold to focus only on the most tuned units\n",
    "\n",
    "valid_neuron_indices = np.nonzero(valid_neurons)[0]\n",
    "\n",
    "print('A list of tuned units: ', valid_neuron_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without changing\n",
    "\n",
    "neuron_index = 0\n",
    "pref_orientation, orth_orientation = get_pref_and_orth_angle(neuron_pref_orientation[valid_neuron_indices[neuron_index]])\n",
    "print(f'Prefered orientation is {pref_orientation:.0f} and orth orientation is {orth_orientation:.0f}')\n",
    "\n",
    "frequency = 3\n",
    "radius = 6\n",
    "c = 0.5\n",
    "\n",
    "contrasts_orth = np.linspace(0.1, 0.5, 10)\n",
    "contrast_pref = np.linspace(c, c, 1)\n",
    "gratings = create_superimposed_gratings(frequency, radius, contrast_pref, contrasts_orth, pref_orientation, orth_orientation, size=16, center=(8, 8))\n",
    "plot_image_row(gratings, titles=[f'contrast: {ctr:.2f}' for ctr in contrasts_orth])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without changing\n",
    "\n",
    "neuron_index = 0\n",
    "pref_orientation, orth_orientation = get_pref_and_orth_angle(neuron_pref_orientation[valid_neuron_indices[neuron_index]])\n",
    "\n",
    "print(f'Prefered orientation is {pref_orientation:.0f} and orth orientation is {orth_orientation:.0f}')\n",
    "\n",
    "contrasts_orth = np.linspace(0.1, 0.5, 10)\n",
    "contrasts_pref_sweep = np.linspace(0.1, 0.5, 4)\n",
    "\n",
    "for c in contrasts_pref_sweep:\n",
    "    contrast_pref = np.linspace(c, c, 1)\n",
    "    gratings = create_superimposed_gratings(frequency, radius, contrast_pref, contrasts_orth, pref_orientation, orth_orientation, size=16, center=(8, 8))\n",
    "\n",
    "    model.initialize_states(gratings[0].reshape(3, 256).numpy())\n",
    "\n",
    "\n",
    "    layer_1_activities_per_orientation = np.zeros((len(gratings), 32))\n",
    "    layer_2_activities_per_orientation = np.zeros((len(gratings), 128))\n",
    "\n",
    "    for i, g in enumerate(gratings):\n",
    "        #the model is expecting a flattened representation of the image\n",
    "        er1, er2, l1, l2 = model(g.reshape(3, 256).numpy())\n",
    "        layer_1_activities_per_orientation[i] = abs(l1).mean(0)\n",
    "        layer_2_activities_per_orientation[i] = abs(l2)\n",
    "\n",
    "    m = layer_1_activities_per_orientation[:, [valid_neuron_indices[neuron_index]]]\n",
    "\n",
    "    _=plt.plot(contrasts_orth, m, label=f'Contrast: {c:.2f}')\n",
    "\n",
    "plt.xlabel('Contrast Values of Orthogonal Orientation')\n",
    "plt.ylabel('Neuron Response')\n",
    "plt.legend(title='Contrast of preferred grating')\n",
    "\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment 11** Do neurons in layer 1 show cross-orientation suppression?\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary style=\"color: green; font-weight: bold;\">Hint</summary>\n",
    "\n",
    "Cross-orientation suppression was mentioned in class in \"Non-linear computations in V1\", Slide 40 of Lecture 5. This model also shows cross-orientation suppression effects for neurons in Layer 1(the response decreases as we add an orthogonal grating).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. End-inhibiton <a class=\"anchor\" id=\"end_inh_rao\"></a>\n",
    "\n",
    "This effect was mentioned in class in \"Predictive coding explains tuning for stimulus size\" slide 50 of Lecture 5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without changing\n",
    "\n",
    "# Set parameters\n",
    "size = 16\n",
    "center = (8,8)\n",
    "contrast = 2\n",
    "width = 1\n",
    "\n",
    "lengths = np.linspace(1, 16, 10)\n",
    "\n",
    "bars = create_bar_lengths(size, contrast, center, lengths, width)\n",
    "plot_image_row(bars, titles=[f'lengths: {l:.0f}' for l in lengths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without changing\n",
    "\n",
    "model.initialize_states(bars[0].reshape(3, 256).numpy())\n",
    "\n",
    "layer_1_activities_per_contrast = np.zeros((len(bars), 32))\n",
    "layer_2_activities_per_contrast = np.zeros((len(bars), 128))\n",
    "\n",
    "for i, g in enumerate(bars):\n",
    "    #the model is expecting a flattened representation of the image\n",
    "    er1, er2, l1, l2 = model(g.reshape(3, 256).numpy())\n",
    "    layer_1_activities_per_contrast[i] = abs(l1).mean(0)\n",
    "    layer_2_activities_per_contrast[i] = abs(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without changing\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot for layer 1 neurons\n",
    "ax1.plot(lengths, layer_1_activities_per_contrast)\n",
    "ax1.set_xlabel('Bar Lengths')\n",
    "ax1.set_ylabel('Average Activity')\n",
    "ax1.set_title('Mean response for Layer 1 neurons')\n",
    "\n",
    "# Plot for layer 2 neurons\n",
    "ax2.plot(lengths, layer_2_activities_per_contrast)\n",
    "ax2.set_xlabel('Bar Lengths')\n",
    "ax2.set_ylabel('Average Activity')\n",
    "ax2.set_title('Mean response for Layer 2 neurons')\n",
    "\n",
    "# Display the plots side by side\n",
    "plt.tight_layout()  # Adjust spacing to prevent overlap\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment 12** How does the response of a neuron change with stimulus size?\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary style=\"color: green; font-weight: bold;\">Hint</summary>\n",
    "\n",
    "For neurons in Layer 1 the response first increases with stimulus size and then it decreases. This is a non-classical receptive field effect. This effect was mentioned in class in \"Surround suppression in V1\" slide 42, \"Multiple contributions to V1 responses\" slide 43 and \"Predictive coding explains tuning for stimulus size\" slide 50 of Lecture 5. Why is this effect seen only in Layer 1 and not also in Layer 2?\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
