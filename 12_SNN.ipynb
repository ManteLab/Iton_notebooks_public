{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<a href=\"https://colab.research.google.com/github/ManteLab/Iton_notebooks_public/blob/main/12_SNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>",
   "id": "a38752aefb2ecc5f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise session 12: Spiking Neural Networks\n",
    "\n",
    "In this exercise session, we will investigate some aspects of spiking neural networks (SNNs). In particular, we first discuss the BCM learning rule, which can be considered an extension of the Hebbian learning rule introduced in lecture 10. Next, we discuss two simple SNNs applications inspired by biology: In part 2 of the exercise, we discuss interaural time difference (ITD) in the case of sound locations observed by the owl. In part 3, we discuss adaptation in vision, which is a mechanism that allows the visual system to adjust its sensitivity to the input."
   ],
   "id": "c571c8158131e389"
  },
  {
   "cell_type": "markdown",
   "id": "c9ab8b63aa924a72",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, we need to update the widgets since we use a never version than the Colab default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6154d8ddfef948b1",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in /Users/sage/opt/anaconda3/envs/phd-playground/lib/python3.11/site-packages (8.1.5)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/sage/opt/anaconda3/envs/phd-playground/lib/python3.11/site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/sage/opt/anaconda3/envs/phd-playground/lib/python3.11/site-packages (from ipywidgets) (8.27.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/sage/opt/anaconda3/envs/phd-playground/lib/python3.11/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /Users/sage/opt/anaconda3/envs/phd-playground/lib/python3.11/site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /Users/sage/opt/anaconda3/envs/phd-playground/lib/python3.11/site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: decorator in /Users/sage/opt/anaconda3/envs/phd-playground/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/sage/opt/anaconda3/envs/phd-playground/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/sage/opt/anaconda3/envs/phd-playground/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /Users/sage/opt/anaconda3/envs/phd-playground/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/sage/opt/anaconda3/envs/phd-playground/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in /Users/sage/opt/anaconda3/envs/phd-playground/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /Users/sage/opt/anaconda3/envs/phd-playground/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.11.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/sage/opt/anaconda3/envs/phd-playground/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Users/sage/opt/anaconda3/envs/phd-playground/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/sage/opt/anaconda3/envs/phd-playground/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/sage/opt/anaconda3/envs/phd-playground/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing in /Users/sage/opt/anaconda3/envs/phd-playground/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /Users/sage/opt/anaconda3/envs/phd-playground/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /Users/sage/opt/anaconda3/envs/phd-playground/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six in /Users/sage/opt/anaconda3/envs/phd-playground/lib/python3.11/site-packages (from asttokens->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U ipywidgets\n",
    "!pip install brian2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378129980432bcf1",
   "metadata": {},
   "source": [
    "Next, we download additional files required by this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bc1bc145b9da8e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T06:43:25.627242Z",
     "start_time": "2024-12-02T06:43:25.620920Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN_COLAB: False\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "print(f\"IN_COLAB: {IN_COLAB}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2b32bc8daff2ac6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T06:43:25.631541Z",
     "start_time": "2024-12-02T06:43:25.628001Z"
    }
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    # Download util scripts\n",
    "    !mkdir utils_ex12\n",
    "    !wget -P utils_ex12/ https://github.com/ManteLab/Iton_notebooks_public/raw/refs/heads/main/utils_ex12/bcm.py\n",
    "    !wget -P utils_ex12/ https://github.com/ManteLab/Iton_notebooks_public/raw/refs/heads/main/utils_ex12/adaptation.py\n",
    "    !wget -P utils_ex12/ https://github.com/ManteLab/Iton_notebooks_public/raw/refs/heads/main/utils_ex12/jeffress.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4733de86ccebf419",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T11:36:17.683374Z",
     "start_time": "2024-12-02T11:36:08.937651Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils_ex12.bcm import *\n",
    "from utils_ex12.adaptation import *\n",
    "from utils_ex12.jeffress import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc7c601d1e4481a",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Artificial Neural Networks (ANNs, see lab from last week), as used in many machine learning applications today, are incredibly powerful tools. However, their learning algorithm backpropagation of error is not biologically plausible and comes with many other downsides like the susceptibility to adversarial attacks, need for large amounts of data, etc.\n",
    "\n",
    "In this lab, we explore Spiking Neural Networks (SNNs), a more biologically plausible model of computation that uses *spikes* to transmit information. Unlike ANNs, SNNs operate in a discrete-time or event-driven paradigm, where spikes occur at specific time steps rather than continuously.\n",
    "\n",
    "#### Learning in Spiking Neural Networks\n",
    "\n",
    "While it is theoretically possible to train SNNs using techniques like backpropagation and surrogate gradients (a concept we will cover in a future lab), this session focuses on **local learning rules**, which are inspired by how biological synapses adapt. Local learning rules:\n",
    "\n",
    "- Do not rely on global error signals.\n",
    "- Use local information, such as pre- and post-synaptic activity, to modify synaptic weights.\n",
    "- Include biologically inspired mechanisms like Spike-Timing Dependent Plasticity (STDP, see lab 10)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81c27764d95040b",
   "metadata": {},
   "source": [
    "# Part 1: BCM Learning Rule\n",
    "\n",
    "\n",
    "The BCM theory, developed by Bienenstock, Cooper, and Munro (hence the name) in 1982, proposes a model of synaptic modification that accounts for experimental observations of neuronal selectivity in the primary sensory cortex and its dependency on input patterns. This theory describes how synaptic changes occur in a biologically plausible manner by linking synaptic plasticity to both presynaptic and postsynaptic activity.\n",
    "\n",
    "Key concepts of this theory includes:\n",
    "\n",
    "- **Hebbian-like Learning:** Synaptic changes depend on the interaction between the presynaptic activity and a *nonlinear function* of the postsynaptic activity.\n",
    "- **Sliding Modification Threshold:** The model introduces a dynamically varying threshold, $\\theta_M$, which governs whether synaptic change is potentiation (strengthening) or depression (weakening). This threshold adjusts based on the previous activity of the postsynaptic neuron.\n",
    "- **Activity Dependency:** The learning rule $\\phi(y; \\theta_M)$, which determines synaptic change, takes the postsynaptic activity $y$ and compares it to the threshold $\\theta_M$:\n",
    "  - When $y < \\theta_M$: The function is negative, leading to synaptic weakening (depression).\n",
    "  - When $y > \\theta_M$: The function is positive, resulting in synaptic strengthening (potentiation).\n",
    "\n",
    "This sliding threshold provides stability to the learning process by enabling competition between incoming patterns rather than relying on converging afferents. This mechanism allows the model to dynamically adapt to changing input statistics and maintain balance in synaptic strengths.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://raw.githubusercontent.com/ManteLab/Iton_notebooks_public/refs/heads/main/utils_ex12/Figures/BCM_rule.png\" alt=\"The BCM Synaptic Modification Rule\" width=\"400\">\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "  <em>The BCM Synaptic Modification Rule. $y$ denotes the output activity of the neuron, $\\theta_M$ is the modification threshold. (Image is from Scholarpedia)</em>\n",
    "</p>\n",
    "\n",
    "### Math\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://www.fabriziomusacchio.com/assets/images/posts/nest/bcm_scheme.png\" alt=\"A neuron with multiple inputs\" width=\"400\">\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "  <em>A single neurons with multiple inputs and one output (Image is from https://www.fabriziomusacchio.com)</em>\n",
    "</p>\n",
    "\n",
    "Lets define $x_i$ as the presynaptic activity of the $i$-th neuron, $y$ as the postsynaptic activity, and $w_i$ as the synaptic weight from $x_i$ to $y$.\n",
    "\n",
    "The change in synaptic weight $dw_i/dt$ is given by:\n",
    "\n",
    "$$\n",
    "    \\frac{dw_i}{dt} = \\eta \\cdot x_i \\cdot y \\cdot (y - \\theta_M)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\eta$: Learning rate.\n",
    "- $x_i$: Presynaptic activity of neuron $i$.\n",
    "- $y$: Postsynaptic activity.\n",
    "- $\\theta_M$: The modification threshold.\n",
    "\n",
    "The modification threshold $\\theta_M$ is a function of the average postsynaptic activity $\\langle y \\rangle$:\n",
    "\n",
    "$$\n",
    "    \\theta_M = \\phi \\cdot \\langle y \\rangle^p, \\text{, with } p > 1\n",
    "$$\n",
    "\n",
    "Here, $\\phi$ is a scaling factor hat adjusts the sensitivity of $\\theta_M$ to $\\langle y \\rangle$. and $p$ is an power factor to ensure a super-linear relationship.\n",
    "The choice of $p>1$ ensures that the threshold $\\theta_M$ grows faster than $\\langle y \\rangle$. This has important implications:\n",
    "\n",
    "1. **Activity-Dependent Stability:** High postsynaptic activity leads to a rapid increase in $\\theta_M$, which in turn makes it harder for future activity to exceed $\\theta_M$. This prevents runaway potentiation, where synapses become excessively strong.\n",
    "2. **Adaptive Plasticity:** Lower activity causes $\\theta_M$ to drop more gradually, allowing for synaptic depression to occur more frequently in underactive synapses.\n",
    "\n",
    "To provide a complete mathematical model, we consider the dynamics of the time-averaged postsynaptic activity $\\langle y \\rangle$ which evolves according to:\n",
    "\n",
    "$$\n",
    "    \\tau \\frac{d\\langle y \\rangle}{dt} =  - \\langle y \\rangle + y\n",
    "$$\n",
    "\n",
    "This equation describes the dynamics of $\\langle y \\rangle$.\n",
    "\n",
    "-  $-\\langle y \\rangle$: This term causes  $\\langle y \\rangle$ to decay toward zero over time if there is no input ($y = 0$).\n",
    "-  $+y$: This term introduces the influence of the instantaneous activity  $y$ on  $\\langle y \\rangle$.\n",
    "-  $\\frac{d\\langle y \\rangle}{dt}$: The rate of change of the time-averaged activity.\n",
    "-  $\\tau$: The time constant, which controls how quickly the time-averaged activity  $\\langle y \\rangle$ adjusts to changes in  $y$. Larger  $\\tau$ values mean slower adjustments, while smaller  $\\tau$ values allow for faster adaptation.\n",
    "\n",
    "\n",
    "##### Intuition Behind the Formula\n",
    "\n",
    "The threshold $\\theta_M$ serves as a self-regulating mechanism for the neuron, ensuring that the balance between potentiation and depression adapts to the neuron's firing rate.\n",
    "- For example:\n",
    "  - If $\\langle y \\rangle$ (average activity) is **low**, $\\theta_M$ will also be low, making it easier for postsynaptic activity $y$ to exceed $\\theta_M$, resulting in potentiation.\n",
    "  - If $\\langle y \\rangle$ is **high**, $\\theta_M$ rises sharply due to the super-linear term ($\\langle y \\rangle^p$), leading to more synaptic depression.\n",
    "\n",
    "This adaptive property allows neurons to stabilize their synaptic strengths while remaining sensitive to changes in activity patterns, a key feature of biologically plausible learning mechanisms.\n",
    "\n",
    "## Weight Decay\n",
    "\n",
    "In the literature, various extensions and refinements of the BCM rule have been proposed, incorporating additional factors such as metaplasticity, homeostatic regulation, and network interactions. These modifications aim to capture the complex interplay of factors influencing synaptic plasticity in neural circuits.\n",
    "\n",
    "A variant of the BCM rule adds a weight decay term $-\\epsilon w_i$ to the Hebbian-like term $y \\cdot (y - \\theta_M) \\cdot x_i$, instead of multiplying it by the learning rate, allowing for a more nuanced regulation of synaptic changes over time:\n",
    "\n",
    "$$\n",
    "\\frac{dw_i}{dt} = y \\cdot (y - \\theta_M) \\cdot x_i - \\epsilon w_i\n",
    "$$\n",
    "\n",
    "The weight decay term acts to stabilize the weight, preventing it from growing indefinitely. This modified BCM rule is different from the previous BCM formulation in that it explicitly includes a mechanism to reduce the synaptic weight over time, proportional to its current value. This is a common modification in neural network models to ensure weights do not grow without bound and to introduce a form of weight regularization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66923a9312bd5f7e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Assignment 1:** Consider a network where most neurons fire frequently ($y$ is consistently high) versus a network where only a few neurons fire occasionally ($y$ is consistently low). What happens to the sliding threshold $\\theta_M$ in both cases and how does this affect synaptic potentiation and depression?  \n",
    "\n",
    "**Assignment 2:** Analyze the behavior of the synapse when $\\theta_M$ is (1.) very high compared to typical postsynaptic activity ($\\theta_M \\gg y$) and (2.) Very low compared to typical postsynaptic activity ($\\theta_M \\ll y$). What happens to the balance between potentiation and depression in each case? How does this affect the synaptic weights?\n",
    "\n",
    "**Assignment 3:** The modified BCM rule introduces a weight decay term $-\\epsilon w_i$. Think about a scenario where $\\epsilon = 0$ (no decay). What happens to the synaptic weights over time?  How does adding weight decay contribute to network stability?  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545c2e6462436517",
   "metadata": {},
   "source": [
    "## Simulation\n",
    "\n",
    "In the following interactive simulation, you can explore the behavior of the BCM learning rule. We use two input neurons $x_1$ and $x_2$ that generate random input patterns. The synaptic weights $w_1$ and $w_2$ are modified according to the BCM rule based on the postsynaptic activity $y$ and the sliding threshold $\\theta_M$.\n",
    "\n",
    "You can adjust the following parameters:\n",
    "\n",
    "- **Frequency Input 1/2**: Frequency of the input signals (pulses).\n",
    "- **Amplitude Input 1/2**: Amplitude of the input signals.\n",
    "- **Phase Shift**: Phase shift between the input signals.\n",
    "- **Use Weight Decay:** Whether to include weight decay in the learning rule.\n",
    "\n",
    "Optionally, you can also adjust the following parameters in the second tab. However, we recommend keeping the default values:\n",
    "- **Learning Rate ($\\eta$):** Determines the speed of synaptic changes.\n",
    "- **Time Constant ($\\tau$):** Time constant for averaging postsynaptic activity.\n",
    "- **Decay Rate ($\\epsilon$):** Rate of weight decay.\n",
    "- **Simulation Time:** Total simulation time in ms.\n",
    "\n",
    "As output, you see the following plots:\n",
    "- **Presynaptic Inputs:** Input signals generated by the two neurons.\n",
    "- **Postsynaptic Activity and Sliding Threshold:** Postsynaptic activity $y$ and the sliding threshold $\\theta_M$.\n",
    "- **Evolution of Synaptic Weights:** Synaptic weights $w_1$ and $w_2$ over time.\n",
    "- **Cumulative Inputs:** Cumulative effect of inputs over time.\n",
    "\n",
    "\n",
    "Note that we limit the weights to the range $[0, 1]$ to ensure they remain within a reasonable range. Without weight decay, the weights can grow without bound, leading to instability (*Hint: This information might be useful to answer the assignments above...*).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7a621c255f9d741",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T06:43:26.788969Z",
     "start_time": "2024-12-02T06:43:26.450132Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d7b9e044f04bd7b8862e70d414b40c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(VBox(children=(FloatSlider(value=0.0, description='Input 1 frequency', layout=Layout(width='500p…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iplot_bcm_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4a1b8a4ae1eae",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Assignment 4:** Use the default settings (execute the cell above to reset to default settings). Explain why the postsynaptic activity increases over time.  Why does it start at $0.4$ and reach $0.8$ by the end? Provide a detailed explanation using the relevant formulae.\n",
    "\n",
    "**Assignment 5:** Use the default settings (reset the parameters by executing the cell above). Initially, the synaptic weights increase almost linearly until they reach the upper limit of $1$ (where the weights are clipped). Next, increase the frequency of Input 1 to $8 \\, \\text{Hz}$ and observe the behavior (press update). Notice that the weight $w_1$ grows incrementally, increasing only when Input 1 ($x_1$) is active. Explain why the growth of $w_2$ becomes nonlinear, even though Input 2 ($x_2$)) remains unchanged.\n",
    "\n",
    "**Assignment 6:** Use the default settings (reset the parameters by executing the cell above). Change the amplitude of Input 2 ($x_2$) to  $0.1$ and observe the results (press update). You will notice that $w_1$ grows faster than $w_2$. Then, enable weight decay (press update). What happens to  $w_2$, and why does this behavior occur?\n",
    "\n",
    "**Assignment 7:** Use the default settings (reset the parameters by executing the cell above). Modify the input settings as follows: set the frequency of Input 2 ( $x_2 $) to  $5 \\, \\text{Hz}$, the amplitude of Input 1 ($x_1$) to  $1.0$, and the amplitude of Input 2 ( $x_2$) to  $0.1$. Observe the evolution of the synaptic weight $w_1$ over time and describe how the postsynaptic activity changes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b664b9a672ccea",
   "metadata": {},
   "source": [
    "# Sound Localization with the Jeffress model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39295c12-a025-4660-8497-ee1df82ccb1a",
   "metadata": {},
   "source": [
    "Most animals have two ears.\n",
    "This allows the perception of different arrivals of a sound in time, called interaural time difference (ITD).\n",
    "The [Jeffress model](http://www.scholarpedia.org/article/Jeffress_model) uses the interaural time difference to estimate the direction of sound sources (in 2-dimensions).\n",
    "\n",
    "## Jeffress model\n",
    "\n",
    "The Jeffress model is a neural model that explains how the brain can locate the source of a sound in space using ITDs. Imagine an owl perched on a branch, listening for the scurrying of a mouse below. When the mouse makes a sound, the sound waves reach the owl's two ears at slightly different times. This tiny difference in arrival time, the ITD, is the key to figuring out where the mouse is.\n",
    "\n",
    "Here's how the Jeffress model works:\n",
    "\n",
    "* **Delay Lines:** Think of the owl's auditory system as having a set of \"delay lines\" that transmit the sound signals from each ear to a set of \"coincidence detector\" neurons in the brain.  Each delay line has a slightly different length, introducing a specific time delay.\n",
    "\n",
    "* **Coincidence Detectors:**  These neurons act like tiny logic gates. They only fire when signals from *both* ears arrive at the *same time*.\n",
    "\n",
    "* **Sound Localization:** Imagine the mouse directly before the owl. The sound reaches both ears simultaneously, so the signal travels down the delay lines and activates a coincidence detector in the middle. If the mouse moves to the right, the sound reaches the right ear first. The signal from the right ear has to travel further down the delay lines to find a coincidence detector that also receives the (delayed) signal from the left ear. The location of the activated coincidence detector tells the owl the direction of the sound!\n",
    "\n",
    "[This](https://www.st-andrews.ac.uk/~wjh/pred_prey/jeffress/jeffress.html) shows a nice animation of this effect.\n",
    "\n",
    "To create these \"delay lines,\" we strategically connect neurons between the left and right ears, introducing varying delays in the connections. This arrangement ensures that signals from each ear arrive at different coincidence detector neurons with specific time offsets, creating a \"temporal map\" of the sound's location.\n",
    "\n",
    "The illustration below shows how this is achieved using the Jeffress model network. Notice how the input from the right ear is connected in a reversed order compared to the input from the left ear. This clever wiring creates the delay lines that are crucial for sound localization.\n",
    "\n",
    "![](https://raw.githubusercontent.com/ManteLab/Iton_notebooks_public/refs/heads/main/utils_ex12/Figures/jeffress_network.png)\n",
    "\n",
    "The next image provides a complete picture of the concept. You can see the sound source (the mouse), the owl with its two ears, and the Jeffress model network. In this scenario, a coincidence detector on the right side is activated. This happens because the sound reaches the left ear first, allowing its signal to travel further down the delay lines and meet the signal from the right ear at that specific neuron. This activation pattern tells the owl that the sound is coming from the left.\n",
    "\n",
    "![](https://raw.githubusercontent.com/ManteLab/Iton_notebooks_public/refs/heads/main/utils_ex12/Figures/concept.png)\n",
    "\n",
    "## Assigments - Explore the Jeffress Model\n",
    "\n",
    "The code below simulates the Jeffress model. You can interactively explore how the model responds to different sound source locations and neuron counts using the following sliders:\n",
    "\n",
    "* **`Angle (deg)`:**  Adjust the angle from which the sound arrives. The green arrow in the visualization indicates the sound source direction.\n",
    "* **`# neurons`:** Change the number of neurons in the Jeffress model.  These neurons are visually represented around the owl, each corresponding to a specific sound location. Increasing the number of neurons should improve the accuracy of sound localization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846bdbb9-ca9f-4a3e-a7ce-d6ac62d6c984",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Assignment 8a:**\n",
    "Keeping `# neurons = 30` while changing the `Angle (deg)` to `0` degrees, `90` degrees, and `180` degrees. What do we observe?\n",
    "\n",
    "**Assignment 8b:** What does the `input delay` plot show? Investigate with the angles from 8a.\n",
    "\n",
    "**Assignment 9:** Reduce the `# neurons` and investigate with different angles. What do we observe?\n",
    "\n",
    "**Assignment 10:** The Jeffress model concerns sound localization in two dimensions. What are its limits? To investigate this, switch to the `Investigate limits` tab, where we can input all possible sound sources in two dimensions. Investigate the model, for example, for the angles of `90` degrees and `270` degrees. What do we observe?\n",
    "\n",
    "**Assignment 11:** Can you summarize the limits we found in the last assignment more generally? Think about symmetries in the owl's input. \n",
    "\n",
    "**Assignment 12:** How can this limitation be solved?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33af543bcc7dfb06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T06:58:05.277792Z",
     "start_time": "2024-12-02T06:58:05.255160Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e2782b37e04a358a446ee28464a170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(VBox(children=(IntSlider(value=0, description='Angle (deg)', max=180, step=15), IntSlider(value=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jeffress_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdc50c2599acccd",
   "metadata": {},
   "source": [
    "# Adaptation in Vision\n",
    "\n",
    "Visual adaptation refers to the ability of the visual system to adjust its sensitivity to changes in light or stimulus intensity. This process allows the eyes to maintain optimal perception under varying lighting conditions, enhancing contrast detection and reducing visual overload. In neural terms, visual adaptation can be modeled as a gradual change in the response of a neuron to stimuli, where repeated or prolonged exposure to a stimulus leads to a reduction in its response. This mechanism is essential for perceiving the environment in a way that remains stable despite variations in input. \n",
    "\n",
    "In the context of a neuron model, visual adaptation can be incorporated by modifying the neuron's firing behavior in response to repeated stimuli. The neuron gradually becomes less sensitive to constant or repetitive inputs, similar to how visual sensitivity decreases under constant light exposure. This adaptation can be modeled by adding an adaptation current to the standard leaky integrate-and-fire (LIF) model, where the current gradually builds with each spike, reducing the neuron's firing rate over time.\n",
    "\n",
    "In the following, we try to understand this adaptation mechanism. Therefore, we first discuss the standard LIF model before extending it with an adaptation term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8b249c815e5d8b",
   "metadata": {},
   "source": [
    "## Leaky Integrate-and-Fire Neuron\n",
    "\n",
    "Before we discuss adaptation, we will introduce the Leaky Integrate-and-Fire (LIF) neuron model. The LIF neuron is a simple model that describes the dynamics of a neuron's membrane potential. The model consists of two main components:\n",
    "\n",
    "### Mathematical Description of the LIF Model\n",
    "The LIF neuron is governed by the following dynamics:\n",
    "\n",
    "1. **Membrane Potential Dynamics**:\n",
    "   The change in membrane potential $V(t)$ is given by:\n",
    "\n",
    "$$\n",
    "   \\tau_m \\frac{dV(t)}{dt} = -(V(t) - V_L) + \\frac{I(t)}{g_L}\n",
    "$$\n",
    "\n",
    "   Where:\n",
    "   - $\\tau_m$: Membrane time constant [ms].\n",
    "   - $V_L$: Leak reversal potential [mV].\n",
    "   - $g_L$: Leak conductance [nS].\n",
    "   - $I(t)$: Input current [pA].\n",
    "\n",
    "\n",
    "The equation models how the voltage across a neuron's membrane, $V(t)$, changes over time.  The left side of the term $\\tau_m \\frac{dV(t)}{dt}$\n",
    "term represents how quickly the membrane potential $V(t)$ changes over time. The speed of this change depends on the **membrane time constant**, $\\tau_m$, which controls how fast the neuron responds to inputs. A larger $\\tau_m$ means the neuron reacts more slowly, smoothing out rapid changes, while a smaller $\\tau_m$ allows the neuron to respond more quickly.\n",
    "\n",
    "The term on the right side $-(V(t) - V_L) + \\frac{I(t)}{g_L}$ describes the two main forces acting on $V(t)$:\n",
    "\n",
    "- **Leak Term: $-(V(t) - V_L)$**: This term pulls the membrane potential back toward a steady \"resting value,\" $V_L$.\n",
    "   - If $V(t)$ is above $V_L$, the term is negative, pushing $V(t)$ down.\n",
    "   - If $V(t)$ is below $V_L$, the term is positive, pulling $V(t)$ up.\n",
    "\n",
    "- **Input Current: $\\frac{I(t)}{g_L}$**:  This term represents external stimulation to the neuron in the form of an input current, $I(t)$, divided by the leak conductance, $g_L$. \n",
    "   - $I(t)$: Describes how much external energy is pushing the membrane potential up.\n",
    "   - $g_L$: Dampens the effect of $I(t)$ based on the neuron's \"leakiness\" (how easily it allows ions to flow).\n",
    "\n",
    "Thus, very simply put, the first part of the equation tries to keep the membrane potential near a stable resting value $V_L$ while the second part pushes it up or down based on the input current.\n",
    "\n",
    "\n",
    "2. **Spike Generation**:\n",
    "   - When $V(t) \\geq V_{th}$ (threshold potential), the neuron \"spikes,\" and:\n",
    "     - $V(t)$ is reset to $V_{reset}$.\n",
    "     - A refractory period $t_{ref}$ prevents spiking immediately after.\n",
    "\n",
    "\n",
    "3. **Reset Dynamics**:\n",
    "   During the refractory period, $V(t)$ is clamped to $V_{reset}$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deae74c94fc96db3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Assignment 13**: Assume you have these parameters given: $V_L = -70mV$ and $V(t) = -60mV$, while the input current is almost $0$. What happens with the membrane potential?\n",
    "\n",
    "**Assignment 14**: Identical to assignment 13, the parameters $V_L = -70mV$ and $V(t) = -60mV$ are given but the input current is very high. What happens with the membrane potential?\n",
    "\n",
    "**Assignment 15**: Assume you make a measurement of the spike time. The initial membrane potential is equal to the reset potential. Then you apply a constant input current. You measure a spike after $5$ms before the membrane potential is reset. When you keep the input current constant - When will the next spike occur according to the LIF model?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f5a717abd92ac4",
   "metadata": {},
   "source": [
    "## Interactive Exploration\n",
    "\n",
    "Modify the parameters below to observe their impact on the behavior of the LIF neuron:\n",
    "\n",
    "- **Input current (`I`)**: Adjust the strength of the stimulus.\n",
    "- **Threshold (`V_th`)**: Make the neuron more or less sensitive.\n",
    "- **Reset potential (`V_reset`)**: Change how much the neuron \"resets\" after a spike.\n",
    "- **Membrane time constant (`tau_m`)**: Control how quickly the neuron integrates input.\n",
    "- **Refractory period (`tref`)**: Set the time the neuron is inactive after spiking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77f14dbfdf5488d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06cf161d31f34db4bc04bcb5a4d0b36b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=-55.0, description='Threshold (V_th)', layout=Layout(width='500px'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "simulate_lif_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61bd054c18d79e3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Assignment 16**: For the LIF simulation above, adjust all parameters (threshold, reset potential, membrane time constant, input current, and refractory period) and describe what you observe.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80be51bfe92bfe4a",
   "metadata": {},
   "source": [
    "## Adaptation in Neurons\n",
    "\n",
    "\n",
    "We now extend the LIF model by introducing an *adaptation current* $w(t)$, which modifies the membrane potential dynamics. Adaptation introduces a slow, spike-triggered feedback mechanism that reduces spiking over time, mimicking biological processes such as ion channel inactivation or hyperpolarization currents.\n",
    "\n",
    "### Mathematical Description of the LIF Model with Adaptation\n",
    "\n",
    "1. **Adaptation Current Dynamics**:\n",
    "   The adaptation current $w(t)$ evolves according to:\n",
    "\n",
    "   $$\n",
    "   \\tau_w \\frac{dw(t)}{dt} = -w(t) + b \\cdot S(t)\n",
    "   $$\n",
    "\n",
    "   Where:\n",
    "   - $\\tau_w$: Adaptation time constant [ms], controlling how slowly or quickly $w(t)$ changes.\n",
    "   - $b$: Increment to the adaptation current per spike [pA], determining the strength of adaptation.\n",
    "   - $S(t)$: Spike indicator function, equal to 1 when $V(t) \\geq V_{th}$ (the neuron spikes) and 0 otherwise.\n",
    "\n",
    "   This equation ensures that:\n",
    "   - When the neuron spikes ($S(t) = 1$), $w(t)$ increases by $b$.\n",
    "   - Between spikes ($S(t) = 0$), $w(t)$ decays back toward zero with a time constant $\\tau_w$.\n",
    "\n",
    "2. **Membrane Potential Dynamics with Adaptation**:\n",
    "   The membrane potential dynamics are now influenced by $w(t)$, modifying the input current $I(t)$:\n",
    "\n",
    "   $$\n",
    "   \\tau_m \\frac{dV(t)}{dt} = -(V(t) - V_L) + \\frac{I(t) - w(t)}{g_L}\n",
    "   $$\n",
    "\n",
    "   Compared to the standard LIF model, the term $w(t)$ subtracts from the input current $I(t)$, effectively reducing the driving force that increases $V(t)$. This creates a spike frequency adaptation effect, where spiking slows down after sustained activity.\n",
    "\n",
    "Intuitively, the adaptation current $w(t)$ acts as a negative feedback mechanism, growing stronger after each spike ($b$) and decaying slowly over time ($\\tau_w$). It also inhibits further spiking by opposing the input current $I(t)$, reducing the membrane potential increment $\\frac{I(t) - w(t)}{g_L}$.\n",
    "\n",
    "The combined dynamics of $V(t)$ and $w(t)$ ensure that initially, the neuron fires frequently in response to strong input. Over time, as $w(t)$ builds up, spiking slows down, mimicking the adaptation observed in biological neurons.\n",
    "\n",
    "\n",
    "3. **Spike Generation and Reset Dynamics**:\n",
    "- When $V(t) \\geq V_{th}$, the neuron spikes, and:\n",
    "   - $V(t)$ is reset to $V_{reset}$.\n",
    "   - $w(t)$ is incremented by $b$, increasing the adaptation current.\n",
    "   - A refractory period $t_{ref}$ follows, during which $V(t)$ is clamped to $V_{reset}$.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb99204fa9447443",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Assignment 17**: In the extended LIF model, the adaptation current $w(t)$ grows larger with each spike and decays slowly over time. Explain intuitively how this mechanism affects the neuron's firing behavior. Why might this be useful for a neuron in a biological system?\n",
    "\n",
    "**Assignment 18**: The parameters $\\tau_w$ (adaptation time constant) and $b$ (adaptation increment per spike) control how adaptation works.  \n",
    "- If $\\tau_w$ is very small, what effect would this have on spiking behavior?  \n",
    "- If $b$ is very large, how would the neuron's firing pattern change over time? \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38162f859099432f",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "### Task Description\n",
    "\n",
    "The adjustable parameters allow you to simulate the effects of neuronal adaptation and observe its impact on the neuron's spiking behavior and adaptation current. This can be related to how sensory systems, like the retina, adjust to changing environments (e.g., adapting from bright light to darkness).\n",
    "\n",
    "You can adjust the following parameters:\n",
    "\n",
    "1. **Pattern Duration (`duration_slider`)**: Controls the period of the input current's sinusoidal pattern, representing flickering light.\n",
    "   - **Effect**: A smaller value increases the flicker rate, while a larger value slows it down.\n",
    "\n",
    "2. **Input Current Factor (`input_current_slider`)**: Scales the amplitude of the input current, mimicking changes in stimulus intensity.\n",
    "   - **Effect**: Larger values increase the neuron's excitation, leading to more frequent spiking.\n",
    "\n",
    "3. **Adaptation Increment (`b_slider`)**: Adjusts the increase in adaptation current (`w`) after each spike.\n",
    "   - **Effect**: Larger values enhance adaptation, reducing the firing rate after repeated spiking.\n",
    "\n",
    "4. **Adaptation Time Constant (`tau_w_slider`)**: Determines how quickly the adaptation current (`w`) decays back to baseline after a spike.\n",
    "   - **Effect**: Smaller values cause faster recovery, while larger values prolong the effects of adaptation.\n",
    "\n",
    "Based on your settings, you can observe these plots:\n",
    "\n",
    "1. **Input Current Over Time**:  Shows the time-varying sinusoidal input current, corresponding to a flickering light stimulus.\n",
    "   - **Interpretation**: Higher peaks indicate brighter periods, while valleys represent dimmer periods.\n",
    "\n",
    "2. **Spike Trains**: Compares spike timings of the neuron:\n",
    "     - *Without Adaptation*: Spikes are driven only by the input current, without any modulation from adaptation.\n",
    "     - *With Adaptation*: Spikes include the effects of adaptation, showing reduced firing rates after high activity.\n",
    "\n",
    "3. **Adaptation Current (`w`)**:  Displays the adaptation current over time when adaptation is enabled.\n",
    "   - **Interpretation**: Peaks correspond to moments when the neuron spikes, and the current builds up with sustained activity. The decay rate is determined by the adaptation time constant (`tau_w`).\n",
    "\n",
    "\n",
    "By adjusting the parameters, you can simulate the neural mechanisms underlying light adaptation in the retina, where photoreceptors and downstream neurons adapt to maintain sensitivity across a wide range of illumination levels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b40a5698d6e82e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ff5fd2bd35405ebae970d35ee38aa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(FloatSlider(value=35.0, description='Pattern Duration (ms)', layout=Layout(width='500px'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "adaptation_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c1bf885dfc76b5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Assignment 19**: Using the provided interactive simulation, adjust the four sliders to explore the impact of different parameters on the neuron's behavior. Adjust each parameter individually and observe the changes in the input current, spike trains, and adaptation current plots. \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
