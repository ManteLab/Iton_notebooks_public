{"cells":[{"cell_type":"markdown","metadata":{"id":"UgHDbTL_0AMS"},"source":["# Introduction to Neuroinformatics\n","\n","\n","## Exercise session 7 (Part 2): Analyzing Representations in Deep Networks\n","\n","In this last section, we explore how a deep neural network represents data across its different layers. To that end, we will be using techniques similar to what we have used so far for neural recordings. You will explore how the network transforms high-dimensional image data layer by layer, and how representations change in this process.\n","\n","We will be using a artifical network (CNN, convolutional neural network) pre-trained to classify images. We will again be using AlexNet like in a previous exercise.\n"]},{"cell_type":"markdown","source":["---\n","\n","# Table of contents\n","* [Packages](#packages)\n","* [3: Alexnet (Again!)](#alex)\n","  * [3.1: RSA - Representational Similarity Analysis](#rsa)\n","      * [3.1.1: First-order Similarity Matrices](#first)\n","      * [3.1.2: Second-order Similarity Matrix](#second)\n","  * [3.2: PCA - Principal Component Analysis](#pca)\n","\n"],"metadata":{"id":"rbBfYOkn0-px"}},{"cell_type":"markdown","source":["---\n","\n","# Packages <a name=\"packages\"></a>\n","The following cells are used to install the necessary packages and libraries for the exercise:\n"],"metadata":{"id":"KAColJG31yho"}},{"cell_type":"code","source":["!which python  # This displays which python is being used\n","!pwd # This displays the current directory"],"metadata":{"id":"4Gb0FLeB168s"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sX0GXlee0AMW"},"outputs":[],"source":["import zipfile\n","from urllib.request import urlretrieve\n","\n","files = [\n","    ('https://github.com/ManteLab/Iton_notebooks_public/raw/refs/heads/main/utils_ex7/utils_2.py', 'utils_2.py'),\n","    ('https://polybox.ethz.ch/index.php/s/MpcSLFUgK3SVXUP/download', 'imagenet.zip')\n","]\n","\n","for url, filename in files:\n","    urlretrieve(url, filename)\n","\n","with zipfile.ZipFile('imagenet.zip', 'r') as zf:\n","    zf.extractall('.')\n","\n","!pip3 install --quiet ipympl"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FZVcCJ5x0AMX"},"outputs":[],"source":["import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from utils_2 import *"]},{"cell_type":"markdown","source":["---\n","\n","# 3. AlexNet (Again!) <a name=\"alex\"></a>"],"metadata":{"id":"4kqeGJgw3Ujm"}},{"cell_type":"markdown","source":["---\n","\n","> **Assignment 21**\n",">\n","> Before we start our journey, what is your intuition of how the dimensionality of the representations changes as we move through the CNNs layers? Should it stay the same, decrease, or increase?\n","\n","Solution:\n","\n","---"],"metadata":{"id":"hTXRRPIw065y"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fp4k_jNv0AMX"},"outputs":[],"source":["convolutional_neural_net = load_cnn_model()"]},{"cell_type":"markdown","metadata":{"id":"1fid1ttg0AMY"},"source":["Alexnet consists of the following layers:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mnBBhOwX0AMY"},"outputs":[],"source":["convolutional_neural_net"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1VVRUk_q0AMY"},"outputs":[],"source":["images, labels = load_imagenet_sample()"]},{"cell_type":"markdown","metadata":{"id":"bah3BjWu0AMZ"},"source":["We have now loaded a batch of images from the dataset. Let's have a look at them. We have four distinct classes: *chain saw*, *french horn*, *gas pump*, and *golf ball* (one row for each).\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"53h1KSaz0AMZ"},"outputs":[],"source":["%matplotlib inline\n","\n","plot_images_grid(images, labels)"]},{"cell_type":"markdown","source":["---\n","\n","> **Assignment 22**\n",">\n","> Looking at these images, what are the features that you think could be useful to classify them into different categories? Think of both low level features (edges, colors), and high level features (shapes, faces, objects).\n","\n","\n","Solution:\n"],"metadata":{"id":"cbqi31dr3Bff"}},{"cell_type":"markdown","metadata":{"id":"rIaweZ9b0AMZ"},"source":["---\n","\n","## 3.1: RSA - Representational Similarity Analysis <a name=\"rsa\"></a>\n","\n","*Representational Similarity Analysis* is a way to compare how \"close\" the representation of data is between (in our case) two different layers of the network. The nice thing about RSA is that it doesn't care about how the units in layer N relate to the units in layer N + 1. RSA doesn't care if the two layers have different number of units. If you want, you can even use RSA to compare two completely different modalities like fMRI data with CNN activation data.\n","\n"]},{"cell_type":"markdown","source":["---\n","\n","### 3.1.1: First-order Similarity Matrices <a name=\"first\"></a>\n","\n","To that end, we first record activity for a fixed set of *n* images/stimuli, and then build an *n* × *n* similarity matrix that shows us how similar the activations were for a given pair of stimuli. We measure similarity here through correlation.\n","\n"],"metadata":{"id":"hoFk8bZq4hCe"}},{"cell_type":"markdown","source":["---\n","\n","> **Assignment 23**\n",">\n","> Why use correlation to measure similarity instead of using e.g. euclidean distance? What might be the advantages of doing it this way?\n","\n","\n","Solution:\n"],"metadata":{"id":"ljlSXA504pWv"}},{"cell_type":"markdown","metadata":{"id":"Pm3A9KKo0AMZ"},"source":["---\n","\n","In order to give the similarity matrices a more interesting and interpretable structure, we will order the images first by class, and within each of the four classes, the first half will be images with high redness, while the second half will have very low redness:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mhGzFfc20AMZ"},"outputs":[],"source":["%matplotlib inline\n","\n","redness_images, redness_labels = pick_redness_extrema(images, labels)\n","plot_images_grid(redness_images, redness_labels)"]},{"cell_type":"markdown","metadata":{"id":"ySZRp0Ab0AMa"},"source":["Now for this data, let us get the activations for each layer, for each image:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ndVhbEVQ0AMa"},"outputs":[],"source":["batch_activations = get_activations_with_names(convolutional_neural_net, redness_images)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pr3xg6kL0AMa"},"outputs":[],"source":["def get_similarity_matrix(batch_activation):\n","    similarity_matrix = np.corrcoef(batch_activation.flatten(start_dim=1))\n","    return similarity_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EPqRre410AMa"},"outputs":[],"source":["similarity_matrices = {\n","    layer_name: get_similarity_matrix(layer_batch_activation) for layer_name, layer_batch_activation in batch_activations.items()\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vBjkxfDo0AMa"},"outputs":[],"source":["%matplotlib inline\n","\n","plot_similarity_matrices_col(similarity_matrices)"]},{"cell_type":"markdown","metadata":{"id":"m0SWSCj70AMa"},"source":["---\n","\n","> **Assignment 24**\n",">\n","> When you look at the at the similarity matrices across layers, are there any interesting changes you can notice? What do they mean?\n","\n","\n","Solution:\n"]},{"cell_type":"markdown","metadata":{"id":"OROaNwGo0AMb"},"source":["---\n","\n","### 3.1.2: Second-order Similarity Matrix <a name=\"second\"></a>\n","\n","The first-order similarity matrices we calculated above give us an insight of how similarly two given stimuli are represented in one layer. Using the fact that for each layer, we have now the same shape *n* × *n* for its similarity matrix, we can now compare layers by comparing their first-order similarity matrices. We again do this in the form of a similarity matrix, this time we call it second-order similarity matrix. As a measure of similarity between two first-order matrices, we will use the Spearman correlation between their entries. This second-order similarity matrix gives us an insight of how representations change between layers.\n","\n"]},{"cell_type":"markdown","source":["---\n","\n","> **Assignment 25**\n",">\n","> What does it mean when two layers have high second-order similarity? What does it mean when they have low similarity?\n","\n","\n","Solution:\n","\n"],"metadata":{"id":"uGStYL2e5Ej2"}},{"cell_type":"markdown","source":["---\n","\n","> **Assignment 26**\n",">\n","> Why do we use correlation for the first-order similarity matrices, but Spearman correlation for the second-order similarity matrix?\n","\n","\n","Solution:\n","\n","\n","---"],"metadata":{"id":"C2bL3TIq5K3O"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ex8HKNVV0AMb"},"outputs":[],"source":["from scipy.stats import spearmanr\n","\n","def spearman_corrcoef(matrix):\n","    corr, _ = spearmanr(matrix, axis=1)\n","    return corr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XAedtlXG0AMb"},"outputs":[],"source":["def extract_lower_tri_entries(matrix):\n","    lower_tri_indices = np.tril_indices_from(matrix, k=-1)\n","    lower_tri_entries = matrix[lower_tri_indices]\n","    return lower_tri_entries.flatten()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B9hEbpuD0AMb"},"outputs":[],"source":["flattened_similarity_matrices = np.array([extract_lower_tri_entries(sm) for sm in similarity_matrices.values()])\n","second_order_similarity_matrix = spearman_corrcoef(flattened_similarity_matrices)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jjx8ijQR0AMb"},"outputs":[],"source":["%matplotlib inline\n","\n","plot_second_order_similarity_matrix(second_order_similarity_matrix, similarity_matrices.keys())"]},{"cell_type":"markdown","metadata":{"id":"v2esmObA0AMb"},"source":["---\n","\n","> **Assignment 27**\n",">\n","> When you look at the second-order similarity matrix, can you see any clear groups of layers?\n","\n","Solution:\n","\n"]},{"cell_type":"markdown","source":["---\n","\n","> **Assignment 28**\n",">\n","> Can you see a pattern how different *types* of layers tend to be similar, or dissimilar?\n","\n","\n","Solution:\n"],"metadata":{"id":"Zl_C8Blj5n0K"}},{"cell_type":"markdown","metadata":{"id":"zPZEN9390AMb"},"source":["---\n","\n","## 3.2: PCA - Principal Component Analysis <a name=\"pca\"></a>\n","\n","Just like we used PCA to analyze neural population responses before, we can apply it again layer by layer to understand how the nature of the representation changes as we go through the layers."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6FdUgN8u0AMc"},"outputs":[],"source":["from numpy.linalg import svd\n","\n","def PCA(data_matrix):\n","    data_matrix = np.array(data_matrix)\n","    data_centered = data_matrix - np.mean(data_matrix, axis=0)\n","    U, S, Vt = svd(data_centered, full_matrices=False)\n","\n","    principal_components = Vt\n","    num_samples = data_matrix.shape[0]\n","    variance_explained = (S ** 2) / (num_samples - 1)\n","    cumulative_variance_explained = np.cumsum(\n","        variance_explained / variance_explained.sum()\n","    )\n","\n","    return principal_components, variance_explained, cumulative_variance_explained"]},{"cell_type":"markdown","metadata":{"id":"MmgbyWzD0AMc"},"source":["Let us do PCA on the original, full batch of images to have sufficient statistics for PCA:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ro4OqARE0AMc"},"outputs":[],"source":["batch_activations = get_activations_with_names(convolutional_neural_net, images, subsample=1000)\n","\n","pcs = {\n","    layer_name: PCA(layer_batch_activation) for layer_name, layer_batch_activation in batch_activations.items()\n","}"]},{"cell_type":"markdown","source":["---\n","\n","> **Assignment 29**\n",">\n","> Here we are subsampling a thousand units in each layer. Why would we do this?\n","\n","\n","Solution:\n","\n","\n","---"],"metadata":{"id":"IsBIFMyn7x0C"}},{"cell_type":"markdown","metadata":{"id":"5lc7PR7c0AMc"},"source":["Explore how much of the variance explained is concentrated in the first few components, for different layers:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pFylEPlI0AMc"},"outputs":[],"source":["plot_variance_explained(pcs)"]},{"cell_type":"markdown","metadata":{"id":"uP6TUO8Q0AMc"},"source":["To have a more quantitative insight into the dimensionality of representations in each layer according to PCA, we compute how many principal components are necessary to get to 80% cumulative variance explained, for each layer:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6yy44ONa0AMc"},"outputs":[],"source":["%matplotlib inline\n","\n","dimensionality = {\n","    layer_name: np.argwhere(cum_var_explained > .8).min() for layer_name, (*_, cum_var_explained) in pcs.items()\n","}\n","\n","plt.plot(dimensionality.keys(), dimensionality.values(), label='components needed to reach 80% var. explained')\n","plt.xticks(rotation=45, ha='right')\n","plt.ylim(bottom=0.)\n","plt.legend();"]},{"cell_type":"markdown","source":["---\n","\n","> **Assignment 30**\n",">\n","> What can you say about the dimensionality of each layer, as you go through the network?\n","\n","\n","Solution:\n","\n","\n","---"],"metadata":{"id":"vKKkLjAJ78hA"}},{"cell_type":"markdown","source":["---\n","\n","# End of this exercise session"],"metadata":{"id":"yCAosRxs8Mnj"}}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":0}