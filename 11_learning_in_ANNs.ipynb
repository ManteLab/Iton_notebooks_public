{"cells":[{"cell_type":"markdown","id":"f365777a-629a-4a9b-929e-98c75e32490e","metadata":{"id":"f365777a-629a-4a9b-929e-98c75e32490e"},"source":["# Exercise session 11: Learning in Artificial Neural Networks (ANNs)\n","\n","Welcome to the eleventh exercise session of the intro to neuroinformatics course. In this notebook, we will explore the learning mechanisms behind artificial neural networks.\n","The first part covers the classic single-layer Perceptron and its learning algorithm.\n","The second part introduces modern artificial neural networks and the concept of gradient descent.\n","In the third part, we gain a deeper intuition of what artificial neural networks learn.\n","The fourth part showcases a real-world artificial neural network called a GAN.\n","The final part demonstrates available pre-trained GANs.\n","\n","---"]},{"cell_type":"markdown","id":"f8524a08-38f7-4dfe-a3a8-dfa6ca967653","metadata":{"id":"f8524a08-38f7-4dfe-a3a8-dfa6ca967653"},"source":["# Table of contents\n","\n","* [Packages](#packages)\n","* [1: Perceptron](#1)\n","  * [1.1: Classic, single-layer perceptron](#1.1)\n","  * [1.2: (Linear) Classification](#1.2)\n","  * [1.3: Learning algorithm](#1.3)\n","  * [1.4: Limit or non-linear classification](#1.4)\n","  * [1.5: Solving non-linear data with a perceptron](#1.5)\n","* [2: Neural network, gradient descent, and backpropogation](#2)\n","  * [2.1: (Layer-wise fully connected) neural network](#2.1)\n","  * [2.2: Learning by optimizing a global loss](#2.2)\n","  * [2.3: Gradients and parameter updates](#2.3)\n","  * [2.4: Simulation: Neural network and gradient descent](#2.4)\n","  * [2.5: Backpropogation](#2.5)\n","  * [2.6: Closing](#2.6)\n","* [3: Feature learning](#3)\n","* [4: GANs](#4)\n","  * [4.1: GAN Model](#4.1)\n","  * [4.2: Conditional GANs](#4.2)\n","\n","  ---"]},{"cell_type":"markdown","source":["# Packages <a name=\"packages\"></a>"],"metadata":{"id":"yHQAJHWC1EsE"},"id":"yHQAJHWC1EsE"},{"cell_type":"code","execution_count":null,"id":"7b94a25e-0400-485c-846b-479e477ba34a","metadata":{"scrolled":true,"id":"7b94a25e-0400-485c-846b-479e477ba34a"},"source":["!pip install ipywidgets"],"outputs":[]},{"cell_type":"markdown","id":"51a83e4f-28f1-482e-b3d2-7f3cc6e97ac3","metadata":{"id":"51a83e4f-28f1-482e-b3d2-7f3cc6e97ac3"},"source":["Next, we download additional files required by this tutorial."]},{"cell_type":"code","execution_count":null,"id":"99c20c15-da47-4d86-a270-bf46a12eec04","metadata":{"id":"99c20c15-da47-4d86-a270-bf46a12eec04"},"source":["try:\n","    import google.colab\n","    IN_COLAB = True\n","except ImportError:\n","    IN_COLAB = False\n","print(f\"IN_COLAB: {IN_COLAB}\")"],"outputs":[]},{"cell_type":"code","execution_count":null,"id":"eb8d3242-8623-487a-ba2a-d298b3208da2","metadata":{"id":"eb8d3242-8623-487a-ba2a-d298b3208da2"},"source":["if IN_COLAB:\n","    # Download util scripts\n","    !mkdir utils_ex11\n","    !wget -P utils_ex11/ https://github.com/ManteLab/Iton_notebooks_public/raw/refs/heads/main/utils_ex11/gd_bp.py\n","    !wget -P utils_ex11/ https://github.com/ManteLab/Iton_notebooks_public/raw/refs/heads/main/utils_ex11/gan.py\n","    !wget -P utils_ex11/ https://github.com/ManteLab/Iton_notebooks_public/raw/refs/heads/main/utils_ex11/p.py\n","    !wget -P utils_ex11/ https://github.com/ManteLab/Iton_notebooks_public/raw/refs/heads/main/utils_ex11/gan_pretrain.py\n","    !wget -P utils_ex11/ https://github.com/ManteLab/Iton_notebooks_public/raw/refs/heads/main/utils_ex11/imagenet_classes.py\n","    # Download figures\n","    !mkdir utils_ex11/Figures\n","    !wget -P utils_ex11/ https://github.com/ManteLab/Iton_notebooks_public/raw/refs/heads/main/utils_ex11/Figures/gan.png\n","    !wget -P utils_ex11/ https://github.com/ManteLab/Iton_notebooks_public/raw/refs/heads/main/utils_ex11/Figures/nn.png\n","    !wget -P utils_ex11/ https://github.com/ManteLab/Iton_notebooks_public/raw/refs/heads/main/utils_ex11/Figures/perceptron.png"],"outputs":[]},{"cell_type":"markdown","id":"91a611d7-7d98-45d5-bd50-1c9fbda34385","metadata":{"id":"91a611d7-7d98-45d5-bd50-1c9fbda34385"},"source":["Now we install dependencies"]},{"cell_type":"code","execution_count":null,"id":"4e0f21e1-7971-462b-aa5c-8d6cc1aab164","metadata":{"scrolled":true,"id":"4e0f21e1-7971-462b-aa5c-8d6cc1aab164"},"source":["!pip install mlxtend\n","!pip install ipython\n","!pip install pytorch-pretrained-biggan\n","!pip install nltk"],"outputs":[]},{"cell_type":"code","execution_count":null,"id":"bd90b3ca-fb54-41cf-adc9-9f10c6211b14","metadata":{"id":"bd90b3ca-fb54-41cf-adc9-9f10c6211b14"},"source":["if IN_COLAB:\n","    from google.colab import output\n","    output.enable_custom_widget_manager()"],"outputs":[]},{"cell_type":"code","execution_count":null,"id":"initial_id","metadata":{"ExecuteTime":{"end_time":"2024-11-12T09:23:35.199972Z","start_time":"2024-11-12T09:23:22.900371Z"},"id":"initial_id"},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import jax.numpy as jnp\n","import jax.random as jrandom\n","\n","from utils_ex11.p import *\n","from utils_ex11.gd_bp import *\n","from utils_ex11.gan import *\n","from utils_ex11.gan_pretrain import *"],"outputs":[]},{"cell_type":"markdown","id":"1e59b8a3-b185-4ea1-ab33-0bf253961a7a","metadata":{"id":"1e59b8a3-b185-4ea1-ab33-0bf253961a7a"},"source":["---\n","\n","# 1. Perceptron <a name=\"1\"></a>\n","\n","The Perceptron is considered one of the earliest artificial neural networks and played a significant role in the development of machine learning. Introduced by Frank Rosenblatt in 1958, it was inspired by the idea of mimicking basic biological neural processes. The Perceptron demonstrated that a machine could learn from data using adjustable weights, establishing a framework for early neural network research.\n","\n","---"]},{"cell_type":"markdown","source":["\n","## 1.1: Classic, single-layer Perceptron <a name=\"1.1\"></a>\n","\n","With today's nomenclature, the classic, single-layer Perceptron is a **neural network with a binary activation function (binary neuron), no hidden layer, and a model-specific learning algorithm**.\n","\n","The single-layer Perceptron model is the following:\n","\n","$\\hat{y} = h(\\vec{x} \\cdot \\vec{\\beta}^T) = h(\\sum_i x_i * \\beta_{i})$\n","\n","Where $h$ is the binary activation function, $x$ is the input, and $\\beta$ are the weights (learnable parameters).\n","$\\hat{y}$ is the predicted output ($0$ or $1$) for one of the two classes.\n","\n","Visually (Note: $w$ is $\\beta$):\n","\n","![My Image](https://github.com/ManteLab/Iton_notebooks_public/blob/main/utils_ex11/Figures/perceptron.png?raw=true)\n","\n","$\\vec{x} \\cdot \\vec{\\beta}^T$ is the inner vector product, which is defined as $\\sum_i x_i * \\beta_{i}$.\n","\n","$h$ is the [Heaviside step function](https://en.wikipedia.org/wiki/Heaviside_step_function), which is $1$ if $\\vec{x} \\cdot \\vec{\\beta}^T$ is positive and $0$ if $\\vec{x} \\cdot \\vec{\\beta}^T$ is negative. The inner product $\\vec{x} \\cdot \\vec{\\beta}^T$ is positive if the vector points in the same direction and negative if they point in opposite directions.\n","\n","---"],"metadata":{"id":"EiE9AGCL1eC6"},"id":"EiE9AGCL1eC6"},{"cell_type":"markdown","source":["\n","## 1.2: (Linear) Classification <a name=\"1.2\"></a>\n","\n","The Classic Perceptron learns to discriminate using a linear decision boundary given data.\n","Thus, the model can learn linear classification as shown below.\n","\n","Given an input point ($x_1$, $x_2$), the Perceptron model predicts either the blue class $0$ or the orange class $1$. For example, for the input $x_1=1$ and $x_2=2$, the model predicts $1$ as it is in the orange area. The model was trained on the shown data points."],"metadata":{"id":"_oQsC41_1gbC"},"id":"_oQsC41_1gbC"},{"cell_type":"code","execution_count":null,"id":"05a70406-f6ca-4347-9e79-3cb455b87031","metadata":{"id":"05a70406-f6ca-4347-9e79-3cb455b87031"},"source":["show_linear_model(interactive=False)"],"outputs":[]},{"cell_type":"markdown","id":"dcfce7b7-8314-4de3-a880-08cc1cc18c16","metadata":{"id":"dcfce7b7-8314-4de3-a880-08cc1cc18c16"},"source":["---\n","\n","## 1.3: Learning algorithm <a name=\"1.3\"></a>\n","\n","Perceptrons use a **specific learning algorithm**, which is different from what today's artificial neural networks use.\n","Today's artificial neural networks use gradient descent (see next section).\n","\n","The perceptron learning algorithm:\n","\n","1. Initialize weights ($\\beta$) with $0$ or random values.\n","2. For a labeled data point (input $x$ and label $y$), we predict the (current) model's prediction $\\hat{y}$ and update $\\beta$ the following way:\n","$$\n","\\vec{\\beta} \\leftarrow \\vec{\\beta} + \\eta * (y - \\hat{y}) * \\vec{x}\n","$$\n","3. Repeat step 2. until some termination condition is reached.\n","\n","The update at 2. does nothing if the prediction was correct $y = \\hat{y}$.\n","If the prediction was wrong $y \\not = \\hat{y}$, there are two cases.\n","$y = 1$, meaning we add $r * \\vec{x}$ to $\\beta$, which moves $\\beta$ to point a little bit more into the direction of $x$, pushing the model twards a prediction of $1$.\n","$y = 0$, meaning we add $-1 * r * \\vec{x}$ to $\\beta$, which moves $\\beta$ to point a little bit more into the opposite direction of $x$, pushing the model toward a prediction of $0$. $\\eta$ represents the step size and is chosen by us."]},{"cell_type":"markdown","id":"89f27260-6870-46d4-8d33-20e18a90a6b5","metadata":{"id":"89f27260-6870-46d4-8d33-20e18a90a6b5"},"source":["This cell **demonstrates** how this learning algorithm works. One frame represents one step of the learning algorithm (2.). Increase the `simulation speed` to slow down the animation. Notice how the model does not change if the sample ($x$) is predicted correctly.`Learning Rate` represents $r$. You can change the `data scale (std)` to change the spread of the two classes."]},{"cell_type":"code","execution_count":null,"id":"24bf947a-0e09-4065-b89d-19e151fa7220","metadata":{"id":"24bf947a-0e09-4065-b89d-19e151fa7220"},"source":["show_linear_model(interactive=True)"],"outputs":[]},{"cell_type":"markdown","id":"6f396894-779a-4230-a7de-c87c19a73278","metadata":{"id":"6f396894-779a-4230-a7de-c87c19a73278"},"source":["---\n","\n","## 1.4: Limit or Non-linear classification <a name=\"1.4\"></a>\n","\n","Being a linear model, the perceptron can **only discriminate linearly separable classes**. That means the data must be separable by a hyperplane for the perceptron to work well. In two dimensions, which we can visualize, the data must be separable by a line.\n","In section 1.1.2, this was the case, and the perceptron found the line separating the two classes. However, if we choose a dataset that is not linearly separable, the **single-layer Perceptron fails**. For example, we generate that here using an **XOR-like operation** for the label. So if both $x_1$ and $x_2$ are large or both are small, we have the class $0$; if one is large, we have the class $1$.\n","\n","If we fit the Perceptron on this dataset, the following happens:"]},{"cell_type":"code","execution_count":null,"id":"e5568119-b376-4991-9222-714923241606","metadata":{"id":"e5568119-b376-4991-9222-714923241606"},"source":["show_nonlinear_model(interactive=True)"],"outputs":[]},{"cell_type":"markdown","id":"ad6675fc-e51e-4b23-b5bb-52b1a03a7f0a","metadata":{"id":"ad6675fc-e51e-4b23-b5bb-52b1a03a7f0a"},"source":["---\n","\n","> **Assignment 1**\n",">\n","> Is the final model good? What went wrong? Does more training help? Does more data (more rectangles and circles) help? Use the simulation below to understand better if you find it hard to answer the questions. The method trains for three epochs, meaning it loops over all data points $3$ times.\n","\n","**Solution:**\n","\n","\n","\n","---"]},{"cell_type":"markdown","id":"24bd1d05-3a3b-4886-8f61-d22ddd79e2ab","metadata":{"id":"24bd1d05-3a3b-4886-8f61-d22ddd79e2ab"},"source":["## 1.5: Solving non-linear data with a perceptron <a name=\"1.5\"></a>\n","\n","There are two standard solutions to learning non-linear relationships.\n","\n","1. Add the non-linearity into the inputs using **manual feature engineering**.\n","We can add a new feature, $x_3$, a non-linear combination of the previous features $x_1$ and $x_2$. Using $x_3$, then the model can learn non-linear functions in $x_1$ and $x_2$ through $x_3$. For example, we can manually create $x_3 = x_1 * x_2$ and provide it as an additional input of the Perceptron model. The Perceptron can use this extra input to learn the following function (see code below).\n","\n","2. Make **extra neurons** (called hidden neurons) $z$ between the output $y$ and input $x$ that **learn autonomously a feature representation** given enough data.\n","This can also be done for Perceptrons, using binary neurons and a custom learning algorithm. Today, this is generally done using a **feed-forward neural network**, the successor to Perceptrons, where neurons are customizable using a custom activation function rather than binary and use gradient descent to learn. We will discuss this in the next section.\n","\n","---"]},{"cell_type":"markdown","id":"eb2ea618-2daf-4b71-8471-635fe73daacd","metadata":{"id":"eb2ea618-2daf-4b71-8471-635fe73daacd"},"source":["> **Assignment 2**\n",">\n","> Below, we use the 1. Solution by adding manual feature engineering.\n","In the code, we do not just input $x_1$ and $x_2$ to the model, but also $x_3=x_1*x_2$. Run the code below. What is different? Does the model now work? Why?\n","\n","**Solution:**\n","\n"]},{"cell_type":"code","execution_count":null,"id":"2cb7e823-dbda-46e0-b9f2-7d6896d1bcaa","metadata":{"id":"2cb7e823-dbda-46e0-b9f2-7d6896d1bcaa"},"source":["# 1. Solution\n","\n","show_nonlinear_model(interactive=True, feature_engineering=True)"],"outputs":[]},{"cell_type":"markdown","id":"223df1ce-b187-4121-80a3-ad2fba79aafd","metadata":{"id":"223df1ce-b187-4121-80a3-ad2fba79aafd"},"source":["---\n","\n","# 2. Neural Network, Gradient Descent and Backpropagation <a name=\"2\"></a>\n","\n","## 2.1 (Layer-wise fully connected) neural network <a name=\"2.1\"></a>\n","\n","An (artificial) neural network $g$ is **loosely inspired by biology**.\n","A neural network is a computational graph with nodes and edges.\n","Nodes represent neurons that, given their inputs, calculate an output value, applying a non-linear activation function to the weighted inputs. Edges represent input-output connections between the neurons.\n","\n","Here, we look at the simplest case, a **layer-wise, fully connected network**,\n","where neurons are grouped in layers and layer-wise fully connected through **directed edges**.\n","An input signal $x$ travels sequentially from **layer to layer** to calculate a final output $\\hat{y}$.\n","The figure shows a 1-hidden layer neural network, but more hidden layers can be added. The learnable parameters $\\beta$ would be on the edges and determine how strongly the source influences the destination neuron.\n","Compared to the Perceptron, we have a **hidden layer $\\vec{z}$** and a general activation function $\\phi$.\n","Considering the last exercise,\n","such a network could approximate $x_3$ in a hidden layer neuron $\\vec{z}$ to successfully classify the data.\n","\n","![image.png](https://github.com/ManteLab/Iton_notebooks_public/blob/main/utils_ex11/Figures/nn.png?raw=true)\n","\n","Like the Perceptron, the fully connected neural network learns the parameters $\\beta$ (on the edges).\n","So, the **neurons and neuron connections are fixed**, while the **connection strengths are learned**.\n","Additionally, the parameters are learned using **gradient descent** to optimize a **single, global loss function** on (training) data.\n","\n","---\n"]},{"cell_type":"markdown","source":["## 2.2 Learning by optimizing a global loss <a name=\"2.2\"></a>\n","\n","In deep learning, we choose a global loss function to measure some performance we want to optimize.\n","We will update all parameters to optimize this single, global loss function.\n","\n","For example, in [**supervised learning**](https://en.wikipedia.org/wiki/Supervised_learning), we want to approximate a function $f$ given a bunch of observed inputs $X$ and corresponding outputs $y$.\n","$f$ is the actual but unknown function, while $\\epsilon$ is some noise one can't predict.\n","\n","$$y = f(X) + \\epsilon$$\n","\n","We now want to model a **neural network $g$ that approximates the unknown function $f$**, such that a model's prediction $\\hat{y}$ is close to an actual observed value $y$ given an input $x$.\n","\n","$$y \\approx \\hat{y} = g(x)$$\n","\n","To measure the approximate equal ($\\approx$), we decide on a **loss function**, for example, the squared loss:\n","\n","$$L(y,\\ \\hat{y})=(y - \\hat{y})^2$$\n","\n","For multiple data points, we assume the **data points to be independent**, meaning we can average their individual errors to get the total loss:\n","\n","$$\n","L(y,\\ \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n","$$\n","\n","Given this global loss function, we can define the **best approximation $g^*$** to $f$ as the function that minimizes our metric $L(y,\\ \\hat{y})$ on some (training) data $(X, y)$. This is generally called [Maximum likelihood estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation).\n","In practice, one must check that $g^*$ does not [overfit](https://en.wikipedia.org/wiki/Overfitting) the noise $\\epsilon$, but we ignore this for simplicity.\n","\n","$$\n","g^* = \\underset{g}{\\operatorname{arg\\,min}}\\,\\ L(y,\\ g(X))\n","$$\n","\n","So what did we achieve? **We transformed the learning of a function approximation into an optimization problem**, where many known algorithms exist.\n","In deep learning, this optimization is done through **first-order algorithms like (mini-batch) gradient descent**.\n","Specifically, we use gradient descent to update the learnable parameters $\\beta$ in our neuron network to minimize a loss $L(y, \\hat{y})$.\n","\n","---"],"metadata":{"id":"KH8nwhGj3e2h"},"id":"KH8nwhGj3e2h"},{"cell_type":"markdown","source":["## 2.3 Gradients and Parameter Updates <a name=\"2.3\"></a>\n","\n","Gradient descent works by adjusting the model parameters $\\beta$ in the **direction that minimizes the loss function**.\n","The direction is given by **each parameter's derivative** (gradient) of the loss function (see [multivariate calculus](https://en.wikipedia.org/wiki/Multivariable_calculus)).\n","First, we calculate each parameter's derivative. For example, the derivative $\\frac{\\partial L}{\\partial \\beta^{l}_{i,j}}$ for the learnable parameter $\\beta^{l}_{i,j}$, that connects the $i$ neuron in layer $l$ to the $j$ neuron in layer $l+1$. The derivative tells us how to change $\\beta^{l}_{i,j}$ to improve the loss $L$.\n","Putting all parameter's derivatives together $\\frac{\\partial L}{\\partial \\beta}$ gives us the **direction in the parameter space to update all parameters simultaneously to reduce the loss $L$**. We ignore interactive effects between the parameters (higher-order derivatives) and optimistically **update all parameters $\\beta$ at once**, taking a meaningful step in the direction each would have to be changed in isolation, formally:\n","\n","$$\n","\\beta \\leftarrow \\beta - \\eta \\cdot \\frac{\\partial L}{\\partial \\beta}\n","$$\n","\n","Where $\\eta$ is the learning rate, representing the step size.\n","Applying this optimization iteratively works well in practice for large neural networks.\n","Given $10^6$ learnable parameters, we have $10^6$ first-order derivatives $\\frac{\\partial L}{\\partial \\beta}$ to calculate.\n","The **parameter's derivatives can be calculated efficiently using backpropagation** (see next section).\n","Using the second derivative is infeasible here as $10^{(6+6)}$ second-order derivatives exist.\n","\n","---"],"metadata":{"id":"LS8RcDSG3g7z"},"id":"LS8RcDSG3g7z"},{"cell_type":"markdown","source":["## 2.4: Simulation: Neural network and gradient descent <a name=\"2.4\"></a>\n","\n","Below, we have a **two-hidden layer** (layer-wise fully connected) neural network that we train on some synthetically generated data.\n"],"metadata":{"id":"lvVlpOPv3iGt"},"id":"lvVlpOPv3iGt"},{"cell_type":"code","execution_count":null,"id":"fc8814d865bed24","metadata":{"ExecuteTime":{"end_time":"2024-11-12T09:23:40.641588Z","start_time":"2024-11-12T09:23:35.201686Z"},"id":"fc8814d865bed24"},"source":["iplot_gn_model()"],"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","> **Assignment 3**\n",">\n","> The cell above shows the gradient descent learning algorithm in action. What do we see in the `Model` plot, and what is the `Loss` plot? `Generate Animation`  can take a few minutes, but afterward, we can step through all the frames representing the gradient descent steps ($\n","\\beta \\leftarrow \\beta - \\eta \\cdot \\frac{\\partial L}{\\partial \\beta}\n","$).\n","\n","**Solution:**\n","\n","\n","---"],"metadata":{"id":"tTCt8iZf-mZN"},"id":"tTCt8iZf-mZN"},{"cell_type":"markdown","source":["> **Assignment 4**\n",">\n","> What happens if we reduce or increase the `learning rate`? Why does a (too) small learning rate not work here? Why does a (too) large learning rate not work here?\n","\n","**Solution:**\n","\n","\n","---"],"metadata":{"id":"b9SbSv-o-o8f"},"id":"b9SbSv-o-o8f"},{"cell_type":"markdown","id":"d7009e8d-d76a-422e-b9bf-5678eb7735f6","metadata":{"jupyter":{"is_executing":true},"id":"d7009e8d-d76a-422e-b9bf-5678eb7735f6"},"source":["\n","## 2.5 Backpropagation <a name=\"2.5\"></a>\n","\n","We can calculate the **derivative for each parameter $\\frac{\\partial L}{\\partial \\beta}=[\\frac{\\partial L}{\\partial \\beta^{(0)}_{0,0}, }\\frac{\\partial L}{\\partial \\beta^{(0)}_{0,1}, },...\\frac{\\partial L}{\\partial \\beta^{(1)}_{0,0}, },...]$ efficiently using backpropagation**.\n","Backpropagation goes **backward through the neural network and loss function** to calculate each parameter's derivative while **caching** shared terms. We won't go into the maths here as it is primarily about using the associativity and distributivity of the chain rule to reuse shared terms. Instead, we try to understand backpropagation intuitively using a simple computational graph.\n","Note that the **mechanism stays the same for more complex computational graphs** (like neural networks).\n"]},{"cell_type":"markdown","source":["---\n","\n","> **Assignment 5**\n",">\n","> The following demo runs the forward pass (prediction) through the computational graph first, followed by the backward pass (backpropagation). During the forward pass, the intermediate data results are computed and stored for the backward pass. The gradients for each intermediate result (shared term) and learnable parameter $w$ are calculated during the backward pass. Run the demo. Explain what happens.\n","\n","**Solution:**\n","\n"],"metadata":{"id":"5n0-hd29-4Xz"},"id":"5n0-hd29-4Xz"},{"cell_type":"code","execution_count":null,"id":"7cff5f73-2218-4412-97e7-c0311e21cb93","metadata":{"id":"7cff5f73-2218-4412-97e7-c0311e21cb93"},"source":["backprop()"],"outputs":[]},{"cell_type":"markdown","id":"a89d36ba-2d73-4f25-87bc-db99b5e6b260","metadata":{"id":"a89d36ba-2d73-4f25-87bc-db99b5e6b260"},"source":["---\n","\n","## 2.6 Closing <a name=\"2.6\"></a>\n","\n","Ignoring some technical aspects, such as including [momentum](https://en.wikipedia.org/wiki/Gradient_descent#Momentum_or_heavy_ball_method) and [training batches](https://en.wikipedia.org/wiki/Stochastic_gradient_descent), the **shown mechanism is how artificial neural networks are trained**. We run a forward pass to calculate intermediate values (`data`) and then run backward to the derived version of the network to calculate the gradients (`grad`) for intermediates (e.g., `x1*w1`) for reuse/caching and parameters (e.g., `w1`) (for the later gradient descent step).\n","\n","In practice, we don't have to implement this algorithm ourselves as it can be generalized to any neural network architecture using **[automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation)**.\n","\n","---"]},{"cell_type":"markdown","id":"e7e4d8f097c28de7","metadata":{"id":"e7e4d8f097c28de7"},"source":["# 3. Feature Learning <a name=\"3\"></a>\n","\n","Now that we better understand the mechanism behind neural networks let's try to sharpen our intuition using a popular demo.\n"]},{"cell_type":"markdown","source":["---\n","\n","> **Assignment 6**\n",">\n","> Visit [https://playground.tensorflow.org](https://playground.tensorflow.org). Play around with the demo. Play around with the learning rate, the activation, the number of layers and hidden units as well as the data.\n","\n","---\n"],"metadata":{"id":"7sHJQKln_IPU"},"id":"7sHJQKln_IPU"},{"cell_type":"markdown","source":["\n","> **Assignment 7**\n",">\n","> Visit [this configuration](https://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.00001&regularizationRate=0&noise=0&networkShape=&seed=0.40739&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false) and try to make the network learn a good approximation without changing the data configuration. What did you have to change?\n","\n","**Solution:**\n","\n","\n","---"],"metadata":{"id":"GHrVqWbv_I7f"},"id":"GHrVqWbv_I7f"},{"cell_type":"markdown","id":"aa6a490ab4516136","metadata":{"id":"aa6a490ab4516136"},"source":["# 4. GANs <a name=\"4\"></a>\n","\n","## 4.1 GAN Model <a name=\"4.1\"></a>\n","\n","In this part, we look into more realistic Deep Learning.\n","More concretely, we **train a simple GAN (Generative Adversarial Network) to generate small pictures** of fashion items.\n","\n","[Original Source](https://colab.research.google.com/github/timsainb/tensorflow2-generative-models/blob/master/2.0-GAN-fashion-mnist.ipynb#scrollTo=e5i-LWyFxDLO)\n","\n","A GAN comprises two neural networks: a **generator network** that generates a fake image from random noise and a **discriminator network** that distinguishes generated fake images from actual images from a dataset.\n","Here is an illustration of this concept:\n","\n","![gan.png](https://github.com/ManteLab/Iton_notebooks_public/blob/main/utils_ex11/Figures/gan.png?raw=true)\n","\n","Here, we use the **Fashion-MNIST dataset**, consisting of small 28*28 grayscale images of fashion items.\n","The generator and discriminator are **trained together using backpropagation** on the following loss function:\n","\n","$$\n","L = L_D + L_G\n","$$\n","\n","$L_D$ is the discriminator loss and $L_G$ is the generator loss.\n","\n","<details>\n","<summary>Extra Details</summary>\n","    \n","Formally, they are:\n","\n","$$\n","L_D = \\mathbb{E}_{x \\sim p_{\\text{data}}} \\left[ \\log D(x) \\right] + \\mathbb{E}_{z \\sim p_z} \\left[ \\log (1 - D(G(z))) \\right]\n","$$\n","\n","$$\n","L_G = \\mathbb{E}_{z \\sim p_z} \\left[ \\log D(G(z)) \\right]\n","$$\n","\n","Intuitively, $L_D$ describes how bad the discriminator detects fake images from the generator $G$, whereas $L_G$ describes how good the fake images are (by fooling the discriminator $D$). $p_z$ is a simple distribution to sample from, e.g., a standardized normal distribution. $p_{data}$ is our dataset Fashion-MNIST.\n","One can imagine losses as a game where $G$ wins if the fakes are too good for $D$ to distinguish, and $D$ wins if it is better at distinguishing fakes than $G$ is at generating fakes. $G$ and $D$ are trained together and learn through this adversarial loss to both improve.\n","\n","Another interesting perspective is to see $D$ as a learnable, changing loss function for $G$. Rather than using a fixed loss function that $G$ can start to exploit, the loss function $D$ is trained along $G$ to be competitive with it.\n","Using backpropagation through $D$, we get meaningful gradients on the outputs of $G$, indicating how $G$'s output must be changed to better fool $D$.\n","\n","</details>\n","\n","\n","Run the code cells below. The cell that trains the model should take around 15 minutes (around 20 seconds per epoch).\n","\n"]},{"cell_type":"code","execution_count":null,"id":"77aefce1-a45c-4d5d-b1ba-af15410c1124","metadata":{"id":"77aefce1-a45c-4d5d-b1ba-af15410c1124"},"source":["generator, discriminator = get_models()"],"outputs":[]},{"cell_type":"code","execution_count":null,"id":"3a28a82f-fa9c-4729-996e-605bc6226714","metadata":{"id":"3a28a82f-fa9c-4729-996e-605bc6226714"},"source":["generator.summary()"],"outputs":[]},{"cell_type":"code","execution_count":null,"id":"7621a905-d8fb-45cf-a81a-936145584952","metadata":{"id":"7621a905-d8fb-45cf-a81a-936145584952"},"source":["discriminator.summary()"],"outputs":[]},{"cell_type":"code","execution_count":null,"id":"2402b791-aef6-41b6-8ff3-21c78ff1558d","metadata":{"id":"2402b791-aef6-41b6-8ff3-21c78ff1558d"},"source":["%matplotlib inline\n","losses = plot_gan(generator, discriminator)"],"outputs":[]},{"cell_type":"code","execution_count":null,"id":"8a303276-df0b-487f-ac64-1476ac83c6df","metadata":{"id":"8a303276-df0b-487f-ac64-1476ac83c6df"},"source":["plot_losses(losses)"],"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","> **Assignment 8**\n",">\n","> Investigate the generator and discriminator summaries. They show each layer in the architecture as a row in a table describing the output shapes and the number of parameters. You are not expected to understand the architecture, but we can understand some aspects. How are the generator and discriminator connected? How many total parameters does our architecture have?  \n","\n","**Solution:**\n","\n","\n","---"],"metadata":{"id":"rs3JluaN_cQ1"},"id":"rs3JluaN_cQ1"},{"cell_type":"markdown","source":["\n","> **Assignment 9**\n",">\n","> The cell with the `plot_gan` function shows the output during the training. You can use the slider (after training is completed) to see intermediate-generated images. Investigate the results.\n","\n","**Solution:**\n","\n","\n","---\n"],"metadata":{"id":"k8s5w2TJ_c6w"},"id":"k8s5w2TJ_c6w"},{"cell_type":"markdown","source":["> **Assignment 10**\n",">\n","> Investigate the plot of the loss function.\n","\n","**Solution:**\n","\n","---"],"metadata":{"id":"2E_ZoEXx_dfa"},"id":"2E_ZoEXx_dfa"},{"cell_type":"markdown","id":"f28f2c56a517681f","metadata":{"id":"f28f2c56a517681f"},"source":["\n","\n","\n","## 4.2 Conditional GANs <a name=\"4.2\"></a>\n","\n","In this exercise, we’ll explore a **pre-trained conditional GAN model**, specifically using the **BigGAN model**, to **generate images based on text descriptions**.\n","What is a Conditional GAN?\n","\n","A conditional GAN differs from a non-conditional GAN by adding a “condition” that directs the generator to **produce images based on a given input** (e.g., text or a label). This input acts as a **guide for the generator**. In this exercise, we’ll use BigGAN, a pre-trained conditional GAN, which takes a text input (a class label, like \"dog\" or \"car\") and generates an image matching that description.\n","\n","Here’s how it works:\n","\n","* Input a Text Prompt: BigGAN expects a word or label as input, representing the type of image you want to generate (e.g., \"cat\" or \"mountain\").\n","* Generate an Image: BigGAN will use its learned associations between labels and image features to create an image that reflects the prompt.\n","* Observe the Results: This exercise demonstrates how a conditional GAN can control the content of generated images based on the input condition.\n","\n","\n","\n","The model used: [https://github.com/huggingface/pytorch-pretrained-BigGAN](https://github.com/huggingface/pytorch-pretrained-BigGAN)"]},{"cell_type":"code","execution_count":null,"id":"9dc2fb9f-2fea-4f13-9544-2ac307c78974","metadata":{"id":"9dc2fb9f-2fea-4f13-9544-2ac307c78974"},"source":["gan_pretrain()"],"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","**Assignment 11:** Enter a text label for the type of image you want (e.g., \"horse\"). Note that only a limited of labels are supported. Use \"Generate random...\" to get a random supported label. You can also use the same label multiple times as the generated images differ always starting from a random vector.\n","\n","---"],"metadata":{"id":"ISKVuzKJ_zyv"},"id":"ISKVuzKJ_zyv"},{"cell_type":"markdown","source":["\n","**Assignment 12:** Generate the image and view the result.\n","\n","---"],"metadata":{"id":"IRUKsKnS_0Uz"},"id":"IRUKsKnS_0Uz"},{"cell_type":"markdown","source":["\n","**Assignment 13:** Experiment with different labels to see the variety of images BigGAN can create.\n","Example Labels to Try\n","* Animals: “dog,” “cat,\"\n","* Nature: “mountain”\n","* Objects: “car,” “bicycle,” “piano”\n","\n","---"],"metadata":{"id":"zjvtoVMv_0-7"},"id":"zjvtoVMv_0-7"},{"cell_type":"markdown","source":["# End of this exercise session"],"metadata":{"id":"gW3zAdDX7Nyi"},"id":"gW3zAdDX7Nyi"}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}